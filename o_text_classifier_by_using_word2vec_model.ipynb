{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 方法参考来之：https://www.cnblogs.com/always-fight/p/10159547.html\n",
    "## 2019-11-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_technology = pd.read_csv(\"./NLPData/technology_news.csv\",encoding =\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>,中新网,1月7日电  恰逢CES 2017拉开大幕，却惊闻“AlphaGo升级版”的M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>徐立，商汤科技CEO在谈起本次参展时谈到：“作为一个成立刚刚两年的创业公司，这次参展，一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>正如最后挑战Master的古力在落败后发表的观点：“人类与人工智能共同探索围棋世界的大幕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>,　SenseFace人脸布控的“黑科技”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>本届大展最大的看点，无疑是“被誉为2016全美科技第一神股”英伟达的首次CES主题演讲。...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content\n",
       "0           0  　　,中新网,1月7日电  恰逢CES 2017拉开大幕，却惊闻“AlphaGo升级版”的M...\n",
       "1           1  　　徐立，商汤科技CEO在谈起本次参展时谈到：“作为一个成立刚刚两年的创业公司，这次参展，一...\n",
       "2           2  　　正如最后挑战Master的古力在落败后发表的观点：“人类与人工智能共同探索围棋世界的大幕...\n",
       "3           3                             　,　SenseFace人脸布控的“黑科技”\n",
       "4           4  　　本届大展最大的看点，无疑是“被誉为2016全美科技第一神股”英伟达的首次CES主题演讲。..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_technology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./NLPData/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna() # 取消可能为空的行数\n",
    "# sprots\n",
    "df_sprots   = pd.read_csv(\"./NLPData/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_society\n",
    "df_society = pd.read_csv(\"./NLPData/society_news.csv\",encoding = 'utf-8')\n",
    "df_society = df_society.dropna()\n",
    "\n",
    "#military\n",
    "df_military   = pd.read_csv(\"./NLPData/military_news.csv\", encoding = 'utf-8')\n",
    "df_military   = df_military.dropna()\n",
    "\n",
    "df_internality = pd.read_csv(\"./NLPData/international_news.csv\",encoding = 'utf-8')\n",
    "df_international = df_internality.dropna()\n",
    "\n",
    "df_house = pd.read_csv(\"./NLPData/house_news.csv\",encoding = 'utf-8')\n",
    "df_house = df_house.dropna()\n",
    "\n",
    "df_home = pd.read_csv(\"./NLPData/home_news.csv\",encoding = 'utf-8')\n",
    "df_home = df_home.dropna()\n",
    "\n",
    "df_finance = pd.read_csv(\"./NLPData/finance_news.csv\",encoding = 'utf-8')\n",
    "df_finance = df_finance.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./NLPData/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./NLPData/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　神州专车高考生免费送考公益活动已持续三年，今年将覆盖30余城市，预计服务数千名考生。神州专车自2015年初在全国60大城市上线，以“专业司机、专业车辆”模式，为用户提供安全、舒适、标准化的出行服务。根据罗兰贝格报告，神州专车占据40%的专车市场份额，消费者满意度和安全评分均位居行业首位。\n"
     ]
    }
   ],
   "source": [
    "print (df_car.content.values.tolist()[11778])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopwords ###\n",
    "stopword_list = [k.strip() for k in open(\"./data/stopwords_NLP.txt\",encoding ='utf-8') if k.strip()!= '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1下面的代码用来采集全部的样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取文件里面的全部的csv文件 ###\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sqtime\n",
    "\n",
    "cutWords = []\n",
    "example  = 0\n",
    "t1 = time.time()\n",
    "for root,dirs,files in os.walk(\"./NLPData\"):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.csv':\n",
    "            label    = os.path.splitext(file)[0] # 以文件的名字做为标签区分 #\n",
    "            doc      = pd.read_csv(os.path.join(root,file),encoding = 'utf-8')\n",
    "            contents = doc.content.dropna()\n",
    "            for each_line in contents:\n",
    "                if len(each_line)>20:\n",
    "                    example +=1\n",
    "                    cutWord = [word for word in jieba.lcut(each_line) if (word not in stopword_list and (word.strip()!= ''or word.lstrip()!=''))]\n",
    "                    cutWords.append([\" \".join(cutWord),label])\n",
    "                    if example%10000 == 0:\n",
    "                        t2 = time.time()\n",
    "                        print(\"处理完前%d个样本所花费的时间是%0.2f\"%(example,(t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2采集超过特定长度，数量相同的个分类样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "def read_files(file,cutWords,label,limit,maxs,i=0):\n",
    "    doc     = pd.read_csv(file,encoding = 'utf-8')\n",
    "    content = doc.content.dropna()\n",
    "    for each_line in content:\n",
    "        if len(each_line)>limit:\n",
    "            i = i+1\n",
    "            if i<=maxs:\n",
    "                cutWord = [word for word in jieba.lcut(each_line) if (word not in stopword_list and (word.strip()!= ''or word.lstrip()!=''))]\n",
    "                cutWords.append([\" \".join(cutWord),label])\n",
    "            if i%20000 == 0:\n",
    "                print(\"处理完前%d个样本所花费的时间是%0.2f\"%(i,(time.time()- t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\H155809\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.725 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完前20000个样本所花费的时间是1.00 mins\n",
      "处理完前20000个样本所花费的时间是1.40 mins\n",
      "处理完前40000个样本所花费的时间是1.40 mins\n",
      "处理完前60000个样本所花费的时间是1.40 mins\n",
      "处理完前80000个样本所花费的时间是1.40 mins\n",
      "处理完前100000个样本所花费的时间是1.40 mins\n",
      "处理完前20000个样本所花费的时间是1.80 mins\n",
      "处理完前40000个样本所花费的时间是1.80 mins\n",
      "处理完前60000个样本所花费的时间是1.80 mins\n",
      "处理完前80000个样本所花费的时间是1.80 mins\n",
      "处理完前100000个样本所花费的时间是1.80 mins\n",
      "处理完前20000个样本所花费的时间是2.50 mins\n",
      "处理完前40000个样本所花费的时间是2.50 mins\n",
      "处理完前60000个样本所花费的时间是2.50 mins\n",
      "处理完前20000个样本所花费的时间是3.26 mins\n",
      "处理完前40000个样本所花费的时间是3.26 mins\n",
      "处理完前60000个样本所花费的时间是3.26 mins\n",
      "处理完前80000个样本所花费的时间是3.26 mins\n",
      "处理完前100000个样本所花费的时间是3.26 mins\n",
      "处理完前120000个样本所花费的时间是3.26 mins\n",
      "处理完前140000个样本所花费的时间是3.26 mins\n",
      "处理完前160000个样本所花费的时间是3.26 mins\n",
      "处理完前180000个样本所花费的时间是3.26 mins\n",
      "处理完前20000个样本所花费的时间是3.65 mins\n"
     ]
    }
   ],
   "source": [
    "#calling \n",
    "import os\n",
    "limit = 50   #采集的样本长度大于这个值\n",
    "maxs  = 9000  # 每个总类的样本数量都是maxs \n",
    "cutWords = []\n",
    "for root,dirs,files in os.walk(\"./NLPData\"):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.csv':\n",
    "            label    = os.path.splitext(file)[0] # 以文件的名字做为标签区分 #\n",
    "            file     = os.path.join(root,file)\n",
    "            read_files(file,cutWords,label,limit,maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 保持分词后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 将cutWords 写入local文件##\n",
    "with open(\"./NLPData/cutWords_lists1.txt\",'w',encoding = 'utf-8') as file:\n",
    "    for cutword in cutWords:\n",
    "        file.write(''.join(cutword[0])+'\\n')#cutword[0] ->content\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 获得文本的对应标签写入单独的文件#\n",
    "with open(\"./NLPData/cutWords_labels1.txt\",'w',encoding = 'utf-8') as file:\n",
    "    for cutword in cutWords:\n",
    "        file.write(''.join(cutword[1])+'\\n')#cutword[1] ->label\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4重新加载文本序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutWords_list = []\n",
    "with open(\"./NLPData/cutWords_lists1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    cutWords_list = [k.split() for k in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./NLPData/cutWords_labels1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    cutWords_labels = [k.strip() for k in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 90000, 'sports_news')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cutWords_list),len(cutWords_labels),cutWords_labels[78000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Word2vec模型训练过程 - 16000次花费时间8++hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "### 155/5 =31 mins,每1000次迭代需要0.5hours ###\n",
    "iters = 1000*16 \n",
    "start = time.time()\n",
    "word2vec_model  = Word2Vec(cutWords_list,size = 100,window = 4,iter = iters,min_count = 20)\n",
    "end = time.time()\n",
    "print(\"训练%d个样本,迭代%d次花费的时间是%0.2f\"%(len(cutWords_list),iters,(end - start)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('电动汽车', 0.6200127601623535),\n",
       " ('车', 0.6135563850402832),\n",
       " ('乘用车', 0.5895926356315613),\n",
       " ('汽车产业', 0.5787907838821411)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 简单相似度计算 ### \n",
    "word2vec_model.wv.most_similar(\"汽车\",topn = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('电动汽车', 0.6200621128082275),\n",
       " ('乘用车', 0.594478964805603),\n",
       " ('车', 0.5929100513458252),\n",
       " ('电动车', 0.558305025100708)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 简单相似度计算 ### \n",
    "word2vec_model.wv.most_similar(\"汽车\",topn = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save model ###\n",
    "word2vec_model.save(\"./NLPData/word2vec_text_classifier_model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('品牌', 0.6022969484329224),\n",
       " ('车型', 0.5839118957519531),\n",
       " ('品质', 0.5372676849365234),\n",
       " ('类产品', 0.5302367806434631),\n",
       " ('技术', 0.49774694442749023)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_models = Word2Vec.load(\"./NLPData/word2vec_text_classifier_model.w2v\")\n",
    "word2vec_models.wv.most_similar('产品',topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('品牌', 0.6022969484329224),\n",
       " ('车型', 0.5839118957519531),\n",
       " ('品质', 0.5372676849365234),\n",
       " ('类产品', 0.5302367806434631),\n",
       " ('技术', 0.49774694442749023)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 重新加载模型\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_models = Word2Vec.load(\"./NLPData/word2vec_text_classifier_model.w2v\")\n",
    "word2vec_models.wv.most_similar('产品',topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 特征工程 - 找到每个词对应的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 对于每一篇文章，获取文章的每个分词在word2vec模型中的向量；\n",
    "### 然后每一篇文章中词分词都可以在word2vec模型中的\n",
    "### 相关性向量的求和后的平均数，即此篇文章在word2vec模型中的相关向量\n",
    "def getVector(cutWords,word2vec_models):\n",
    "    count = 0\n",
    "    article_vector = np.zeros(word2vec_models.layer1_size)\n",
    "    for cutWord in cutWords:\n",
    "        if cutWord in word2vec_models:\n",
    "            article_vector+= word2vec_models[cutWord]\n",
    "            count+=1 \n",
    "    return article_vector/count # 返回的是文档中每个词对应词向量的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前5000篇文章所形成词向量花费的时间1.84 sec\n",
      "前10000篇文章所形成词向量花费的时间3.87 sec\n",
      "前15000篇文章所形成词向量花费的时间5.69 sec\n",
      "前20000篇文章所形成词向量花费的时间7.45 sec\n",
      "前25000篇文章所形成词向量花费的时间9.14 sec\n",
      "前30000篇文章所形成词向量花费的时间10.77 sec\n",
      "处理全部花费的时间:10.79 sec\n"
     ]
    }
   ],
   "source": [
    "# 取一部分数量的词来更换成词向量\n",
    "import time\n",
    "cutword_nums = 30000\n",
    "start        = time.time()\n",
    "vector_list  = []\n",
    "i = 0\n",
    "for cutWords in cutWords_list[:cutword_nums]:\n",
    "    vector_list.append(getVector(cutWords,word2vec_models))\n",
    "    i+=1\n",
    "    if i%5000 == 0:\n",
    "        print(\"前%d篇文章所形成词向量花费的时间%0.2f\" %(i,time.time() - start),'sec')\n",
    "X = np.array(vector_list)\n",
    "print(\"处理全部花费的时间:%0.2f\"%(time.time() - start),'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.54689496,  0.64103659, -0.39199093,  1.43781886,  0.6310716 ,\n",
       "         0.76436238, -1.38108583,  0.63742974, -0.72374454, -0.67781096,\n",
       "         2.69448707,  2.05090154, -0.17992053,  1.24570299,  2.88979038,\n",
       "        -0.86781573, -0.84417639, -1.87904089, -0.84036474, -0.09907529,\n",
       "        -0.58167907, -2.05209265,  1.35086218, -1.13256412,  0.4469694 ,\n",
       "         1.39755732, -0.19939938,  0.20436128,  0.55760977,  0.34531186,\n",
       "         1.29542244, -2.96394827, -0.49776433, -3.35187512,  1.98491816,\n",
       "         0.27390966,  1.6542639 ,  0.53025854, -0.46206792,  0.77466084,\n",
       "        -2.79504148, -1.94275436, -1.4236882 ,  0.37690396,  2.2998719 ,\n",
       "        -1.45289868, -1.82754733, -0.68855958,  1.92941608,  0.01898815,\n",
       "         2.4808457 ,  0.888111  , -0.4092853 , -2.74308644,  0.21103184,\n",
       "        -1.59519653,  0.50129806,  2.42840368, -3.96921759, -3.35445194,\n",
       "        -1.76307967,  0.28358459,  0.17493603, -1.14530809, -0.92605421,\n",
       "        -2.05514167, -0.12468004, -0.29593068,  0.05317302, -1.82292755,\n",
       "        -1.29033336,  1.95826953, -1.0365201 ,  0.21733837, -0.40951313,\n",
       "         0.13305273, -1.48648554, -0.78629501, -2.72679777,  0.70935933,\n",
       "         1.08284713, -1.32973501, -0.28039605, -3.26989667,  2.69504917,\n",
       "         1.72433671,  1.07938221, -0.71766332,  0.06784883,  1.55171276,\n",
       "        -1.05243384,  3.95477606, -2.40859265, -0.02362555,  2.74184073,\n",
       "         0.83787408, -0.21341298,  2.06590302,  1.90644612,  0.86459171]), 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### j将计算的特征矩阵save成文档来\n",
    "X.dump('./NLPData/articles_vector.txt') # dump方法传入一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load('./NLPData/articles_vector.txt',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 将label 转成数字标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "labelencoder = LabelEncoder()\n",
    "with open(\"./NLPData/cutWords_labels1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    y_labels = [k.strip() for k in file]\n",
    "y =  labelencoder.fit_transform(y_labels[:cutword_nums]) # 所有的label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),len(y),type(X),type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 数据样本分割 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1样本分割前线判断X或者y里面是不是用空的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any(),np.isnan(y).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[np.isnan(X) == True] = 0  # 将空的值强制为零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 样本数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 200)\n",
    "log_clf = LogisticRegression(multi_class = 'multinomial',solver = 'lbfgs') # ,\n",
    "log_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 200)\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型准确率:86.62 %\n"
     ]
    }
   ],
   "source": [
    "print(\"逻辑回归模型准确率:%0.2f\"%(log_clf.score(X_test,y_test)*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./NLPData/logistic.model']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 通过joblib来保存model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(log_clf,'./NLPData/logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 加载model\n",
    "from sklearn.externals import joblib\n",
    "logistic_model = joblib.load('./NLPData/logistic.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 相同的模型使用交叉验证查看准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86116667  0.86383333  0.868       0.86083333  0.85766667  0.8615      0.854\n",
      "  0.86233333  0.86733333  0.86716667]\n",
      "交叉验证的准确率是86.24 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cv_split        = ShuffleSplit(X.shape[0],train_size = 0.7,test_size = 0.2) # X.shape[0] = 24000  #n_iter default = 10\n",
    "log_model       = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "score_ndarray   = cross_val_score(log_model,X,y,cv = cv_split)\n",
    "print(score_ndarray)\n",
    "print(\"交叉验证的准确率是%0.2f\"%(score_ndarray.mean()*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ==============另一种方法:采用TF-IDF算法提取特征=================###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 重新加载样本和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutWords_list = []\n",
    "with open(\"./NLPData/cutWords_lists1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    cutWords_list = [k.split() for k in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutWords_labels = []\n",
    "with open(\"./NLPData/cutWords_labels1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    cutWords_labels = [k.strip() for k in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 通过Tf-idf vectorize 来抽取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopwords ###\n",
    "stopword_list = [k.strip() for k in open(\"./data/stopwords_NLP.txt\",encoding ='utf-8') if k.strip()!= '']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf         = TfidfVectorizer(cutWords_list,stop_words =stopword_list,min_df = 50,max_df = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "def read_files(file,cutWords,label,limit,maxs,i=0):\n",
    "    doc     = pd.read_csv(file,encoding = 'utf-8')\n",
    "    content = doc.content.dropna()\n",
    "    for each_line in content:\n",
    "        if len(each_line)>limit:\n",
    "            i = i+1\n",
    "            if i<=maxs:\n",
    "                \n",
    "            if i%20000 == 0:\n",
    "                print(\"处理完前%d个样本所花费的时间是%0.2f\"%(i,(time.time()- t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "　/　/本/报/济/南/1/月/2/日/电/ /(/记/者/潘/俊/强/)/日/前/，/备/受/关/注/的/济/南/“/专/车/案/”/一/审/宣/判/。/济/南/市/市/中/区/人/民/法/院/对/济/南/市/民/陈/超/诉/济/南/市/城/市/公/共/客/运/管/理/服/务/中/心/行/政/处/罚/一/案/作/出/一/审/宣/判/，/判/决/撤/销/济/南/市/客/管/中/心/对/“/专/车/”/司/机/陈/超/的/行/政/处/罚/。\n"
     ]
    }
   ],
   "source": [
    "doc         = pd.read_csv(\"./NLPData/car_news.csv\",encoding = 'utf-8')\n",
    "doc.columns = ['index','text']\n",
    "i = 0\n",
    "for one in doc['text']:\n",
    "    i+=1\n",
    "    if i ==1:\n",
    "        ones = \"/\".join(one)\n",
    "        print(type(ones))\n",
    "        print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u3000济南市市中区人民法院审理认为，本案中，陈超在与乘客通过网络约车软件取得联系后，使用未取得运营证的车辆将乘客送至目的地，并按约定收取了车费。陈超的行为构成未经许可擅自从事出租汽车客运经营，违反了现行法律的规定。但虑及网约车这种共享经济新业态的特殊背景，该行为的社会危害性较小。'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 特征工程-抽取特征### \n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 读取特定path下文件夹里面的特定格式的文件，并写入特定文件内###\n",
    "import os \n",
    "import csv \n",
    "import pandas as pd\n",
    "def read_file(path,file,special_format):\n",
    "    writer = open(path,'w',encoding = 'utf-8')\n",
    "    for root,dirs,files in os.walk():\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == special_format:\n",
    "                with open(os.path.join(root,file),'r',encoding = \"utf-8\") as f:\n",
    "                    for line in f.readlines():\n",
    "                        writer.write(line.strip('\\n')+'\\n')\n",
    "    writer.close()\n",
    "#Example:\n",
    "#file = \"./datas/mt/酒店评论数据备份v2/数据分割处理/postive/content/contentMergedPostive.txt\"\n",
    "#path = \"./datas/mt/酒店评论数据备份v2/数据分割处理/postive/content\"\n",
    "#special_format = \".txt\"\n",
    "#read_file(path,file,special_format)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
