{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 方法参考来之：https://www.cnblogs.com/always-fight/p/10159547.html\n",
    "## 2019-11-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_technology = pd.read_csv(\"./NLPData/technology_news.csv\",encoding =\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>,中新网,1月7日电  恰逢CES 2017拉开大幕，却惊闻“AlphaGo升级版”的M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>徐立，商汤科技CEO在谈起本次参展时谈到：“作为一个成立刚刚两年的创业公司，这次参展，一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>正如最后挑战Master的古力在落败后发表的观点：“人类与人工智能共同探索围棋世界的大幕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>,　SenseFace人脸布控的“黑科技”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>本届大展最大的看点，无疑是“被誉为2016全美科技第一神股”英伟达的首次CES主题演讲。...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content\n",
       "0           0  　　,中新网,1月7日电  恰逢CES 2017拉开大幕，却惊闻“AlphaGo升级版”的M...\n",
       "1           1  　　徐立，商汤科技CEO在谈起本次参展时谈到：“作为一个成立刚刚两年的创业公司，这次参展，一...\n",
       "2           2  　　正如最后挑战Master的古力在落败后发表的观点：“人类与人工智能共同探索围棋世界的大幕...\n",
       "3           3                             　,　SenseFace人脸布控的“黑科技”\n",
       "4           4  　　本届大展最大的看点，无疑是“被誉为2016全美科技第一神股”英伟达的首次CES主题演讲。..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_technology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./NLPData/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna() # 取消可能为空的行数\n",
    "# sprots\n",
    "df_sprots   = pd.read_csv(\"./NLPData/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_society\n",
    "df_society = pd.read_csv(\"./NLPData/society_news.csv\",encoding = 'utf-8')\n",
    "df_society = df_society.dropna()\n",
    "\n",
    "#military\n",
    "df_military   = pd.read_csv(\"./NLPData/military_news.csv\", encoding = 'utf-8')\n",
    "df_military   = df_military.dropna()\n",
    "\n",
    "df_internality = pd.read_csv(\"./NLPData/international_news.csv\",encoding = 'utf-8')\n",
    "df_international = df_internality.dropna()\n",
    "\n",
    "df_house = pd.read_csv(\"./NLPData/house_news.csv\",encoding = 'utf-8')\n",
    "df_house = df_house.dropna()\n",
    "\n",
    "df_home = pd.read_csv(\"./NLPData/home_news.csv\",encoding = 'utf-8')\n",
    "df_home = df_home.dropna()\n",
    "\n",
    "df_finance = pd.read_csv(\"./NLPData/finance_news.csv\",encoding = 'utf-8')\n",
    "df_finance = df_finance.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./NLPData/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./NLPData/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　神州专车高考生免费送考公益活动已持续三年，今年将覆盖30余城市，预计服务数千名考生。神州专车自2015年初在全国60大城市上线，以“专业司机、专业车辆”模式，为用户提供安全、舒适、标准化的出行服务。根据罗兰贝格报告，神州专车占据40%的专车市场份额，消费者满意度和安全评分均位居行业首位。\n"
     ]
    }
   ],
   "source": [
    "print (df_car.content.values.tolist()[11778])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopwords ###\n",
    "stopword_list = [k.strip() for k in open(\"./data/stopwords_NLP.txt\",encoding ='utf-8') if k.strip()!= '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1下面的代码用来采集全部的样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取文件里面的全部的csv文件 ###\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "cutWords = []\n",
    "example  = 0\n",
    "t1 = time.time()\n",
    "for root,dirs,files in os.walk(\"./NLPData\"):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.csv':\n",
    "            label    = os.path.splitext(file)[0] # 以文件的名字做为标签区分 #\n",
    "            doc      = pd.read_csv(os.path.join(root,file),encoding = 'utf-8')\n",
    "            contents = doc.content.dropna()\n",
    "            for each_line in contents:\n",
    "                if len(each_line)>20:\n",
    "                    example +=1\n",
    "                    cutWord = [word for word in jieba.lcut(each_line) if (word not in stopword_list and (word.strip()!= ''or word.lstrip()!=''))]\n",
    "                    cutWords.append([\" \".join(cutWord),label])\n",
    "                    if example%10000 == 0:\n",
    "                        t2 = time.time()\n",
    "                        print(\"处理完前%d个样本所花费的时间是%0.2f\"%(example,(t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2采集超过特定长度，数量相同的个分类样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "def read_files(file,cutWords,label,limit,maxs,i=0):\n",
    "    doc     = pd.read_csv(file,encoding = 'utf-8')\n",
    "    content = doc.content.dropna()\n",
    "    for each_line in content:\n",
    "        if len(each_line)>limit:\n",
    "            i = i+1\n",
    "            if i<=maxs:\n",
    "                cutWord = [word for word in jieba.lcut(each_line) if (word not in stopword_list and (word.strip()!= ''or word.lstrip()!=''))]\n",
    "                cutWords.append([\" \".join(cutWord),label])\n",
    "            if i%20000 == 0:\n",
    "                print(\"处理完前%d个样本所花费的时间是%0.2f\"%(i,(time.time()- t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完前10000个样本所花费的时间是0.97 mins\n",
      "处理完前20000个样本所花费的时间是0.97 mins\n",
      "处理完前10000个样本所花费的时间是1.32 mins\n",
      "处理完前20000个样本所花费的时间是1.32 mins\n",
      "处理完前30000个样本所花费的时间是1.32 mins\n",
      "处理完前40000个样本所花费的时间是1.32 mins\n",
      "处理完前50000个样本所花费的时间是1.32 mins\n",
      "处理完前60000个样本所花费的时间是1.32 mins\n",
      "处理完前70000个样本所花费的时间是1.32 mins\n",
      "处理完前80000个样本所花费的时间是1.32 mins\n",
      "处理完前90000个样本所花费的时间是1.32 mins\n",
      "处理完前100000个样本所花费的时间是1.32 mins\n",
      "处理完前110000个样本所花费的时间是1.32 mins\n",
      "处理完前10000个样本所花费的时间是1.67 mins\n",
      "处理完前20000个样本所花费的时间是1.67 mins\n",
      "处理完前30000个样本所花费的时间是1.67 mins\n",
      "处理完前40000个样本所花费的时间是1.67 mins\n",
      "处理完前50000个样本所花费的时间是1.67 mins\n",
      "处理完前60000个样本所花费的时间是1.67 mins\n",
      "处理完前70000个样本所花费的时间是1.67 mins\n",
      "处理完前80000个样本所花费的时间是1.67 mins\n",
      "处理完前90000个样本所花费的时间是1.67 mins\n",
      "处理完前100000个样本所花费的时间是1.67 mins\n",
      "处理完前110000个样本所花费的时间是1.67 mins\n",
      "处理完前10000个样本所花费的时间是2.02 mins\n",
      "处理完前10000个样本所花费的时间是2.30 mins\n",
      "处理完前20000个样本所花费的时间是2.30 mins\n",
      "处理完前30000个样本所花费的时间是2.30 mins\n",
      "处理完前40000个样本所花费的时间是2.30 mins\n",
      "处理完前50000个样本所花费的时间是2.30 mins\n",
      "处理完前60000个样本所花费的时间是2.30 mins\n",
      "处理完前70000个样本所花费的时间是2.30 mins\n",
      "处理完前10000个样本所花费的时间是2.65 mins\n",
      "处理完前10000个样本所花费的时间是3.00 mins\n",
      "处理完前20000个样本所花费的时间是3.00 mins\n",
      "处理完前30000个样本所花费的时间是3.00 mins\n",
      "处理完前40000个样本所花费的时间是3.00 mins\n",
      "处理完前50000个样本所花费的时间是3.00 mins\n",
      "处理完前60000个样本所花费的时间是3.00 mins\n",
      "处理完前70000个样本所花费的时间是3.00 mins\n",
      "处理完前80000个样本所花费的时间是3.00 mins\n",
      "处理完前90000个样本所花费的时间是3.00 mins\n",
      "处理完前100000个样本所花费的时间是3.00 mins\n",
      "处理完前110000个样本所花费的时间是3.00 mins\n",
      "处理完前120000个样本所花费的时间是3.00 mins\n",
      "处理完前130000个样本所花费的时间是3.00 mins\n",
      "处理完前140000个样本所花费的时间是3.00 mins\n",
      "处理完前150000个样本所花费的时间是3.00 mins\n",
      "处理完前160000个样本所花费的时间是3.00 mins\n",
      "处理完前170000个样本所花费的时间是3.00 mins\n",
      "处理完前180000个样本所花费的时间是3.00 mins\n",
      "处理完前190000个样本所花费的时间是3.00 mins\n",
      "处理完前10000个样本所花费的时间是3.36 mins\n",
      "处理完前20000个样本所花费的时间是3.36 mins\n",
      "处理完前10000个样本所花费的时间是3.74 mins\n"
     ]
    }
   ],
   "source": [
    "#calling \n",
    "limit = 50   #采集的样本长度大于这个值\n",
    "maxs  = 8000  # 每个总类的样本数量都是maxs \n",
    "cutWords = []\n",
    "for root,dirs,files in os.walk(\"./NLPData\"):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1] == '.csv':\n",
    "            label    = os.path.splitext(file)[0] # 以文件的名字做为标签区分 #\n",
    "            file     = os.path.join(root,file)\n",
    "            read_files(file,cutWords,label,limit,maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 保持分词后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 将cutWords 写入local文件##\n",
    "with open(\"./NLPData/cutWords_lists1.txt\",'w',encoding = 'utf-8') as file:\n",
    "    for cutword in cutWords:\n",
    "        file.write(''.join(cutword[0])+'\\n')#cutword[0] ->content\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 获得文本的对应标签写入单独的文件#\n",
    "with open(\"./NLPData/cutWords_labels1.txt\",'w',encoding = 'utf-8') as file:\n",
    "    for cutword in cutWords:\n",
    "        file.write(''.join(cutword[1])+'\\n')#cutword[1] ->label\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4重新加载文本序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutWords_list = []\n",
    "with open(\"./NLPData/cutWords_lists1.txt\",'r',encoding = 'utf-8') as file:\n",
    "    cutWords_list = [k.split() for k in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Word2vec模型训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练80000个样本花费的时间是 2.62 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "start = time.time()\n",
    "word2vec_model  = Word2Vec(cutWords_list,size = 100,window = 2,iter = 100,min_count = 20)\n",
    "end = time.time()\n",
    "print(\"训练%d个样本花费的时间是 %0.2f\"%(len(cutWords_list),(end - start)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('电动汽车', 0.6746231913566589),\n",
       " ('电动车', 0.6409035921096802),\n",
       " ('汽车产业', 0.6318580508232117),\n",
       " ('车', 0.6138498783111572)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 简单相似度计算 ### \n",
    "word2vec_model.wv.most_similar(\"汽车\",topn = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save model ###\n",
    "word2vec_model.save(\"./NLPData/word2vec_text_classifier_model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('关键技术', 0.6680781841278076),\n",
       " ('技术创新', 0.6376924514770508),\n",
       " ('核心技术', 0.6248726844787598),\n",
       " ('人工智能', 0.5972033739089966),\n",
       " ('科技', 0.5760294198989868)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 重新加载模型\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_models = Word2Vec.load(\"./NLPData/word2vec_text_classifier_model.w2v\")\n",
    "word2vec_models.wv.most_similar('技术',topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 特征工程 - 找到每个词对应的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 对于每一篇文章，获取文章的每个分词在word2vec模型中的向量；\n",
    "### 然后每一篇文章中词分词都可以在word2vec模型中的\n",
    "### 相关性向量的求和后的平均数，即此篇文章在word2vec模型中的相关向量\n",
    "def getVector(cutWords,word2vec_models):\n",
    "    count = 0\n",
    "    article_vector = np.zeros(word2vec_models.layer1_size)\n",
    "    for cutWord in cutWords:\n",
    "        if cutWord in word2vec_models:\n",
    "            article_vector+= word2vec_models[cutWord]\n",
    "            count+=1 \n",
    "    return article_vector/count # 返回的是文档中每个词对应词向量的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取一部分数量的词来更换成词向量\n",
    "cutword_nums = 50000\n",
    "start        = time.time()\n",
    "vector_list  = []\n",
    "i = 0\n",
    "for cutWords in cutWords_list[:cutword_nums]:\n",
    "    vector_list.append(getVector(cutWords,word2vec_models))\n",
    "    i+=1\n",
    "    if i%5000 == 0:\n",
    "        print(\"前%d篇文章所形成词向量花费的时间%0.2f\" %(i,time.time() - start),'sec')\n",
    "X = np.array(vector_list)\n",
    "print(\"处理全部花费的时间:%0.2f\"%(time.time() - start),'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[0],len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### j将计算的特征矩阵save成文档来\n",
    "X.dump('./NLPData/articles_vector.txt') # dump方法传入一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load('./NLPData/articles_vector.txt',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "labelencoder = LabelEncoder()\n",
    "with open(\"./NLPData/cutWords_labels.txt\",'r',encoding = 'utf-8') as file:\n",
    "    y_labels = [k.strip() for k in file]\n",
    "y =  labelencoder.fit_transform(y_labels) # 所有的label y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 200)\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train,y_train)\n",
    "print(\"逻辑回归模型准确率:%0.2f\"%(log_clf.score(X_test,y_test))*100,\"%\")\n",
    "\n",
    "### 通过joblib来保存model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logistic_model,'./NLPData/logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 加载model\n",
    "from sklearn.externals import joblib\n",
    "logistic_model = joblib.load('./NLPData/logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 读取特定path下文件夹里面的特定格式的文件，并写入特定文件内###\n",
    "import os \n",
    "import csv \n",
    "import pandas as pd\n",
    "def read_file(path,file,special_format):\n",
    "    writer = open(path,'w',encoding = 'utf-8')\n",
    "    for root,dirs,files in os.walk():\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == special_format:\n",
    "                with open(os.path.join(root,file),'r',encoding = \"utf-8\") as f:\n",
    "                    for line in f.readlines():\n",
    "                        writer.write(line.strip('\\n')+'\\n')\n",
    "    writer.close()\n",
    "#Example:\n",
    "#file = \"./datas/mt/酒店评论数据备份v2/数据分割处理/postive/content/contentMergedPostive.txt\"\n",
    "#path = \"./datas/mt/酒店评论数据备份v2/数据分割处理/postive/content\"\n",
    "#special_format = \".txt\"\n",
    "#read_file(path,file,special_format)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
