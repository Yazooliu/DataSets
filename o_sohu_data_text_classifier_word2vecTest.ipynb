{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 下面代码来之：https://www.cnblogs.com/always-fight/p/10159547.html\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0  娱乐  《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...\n",
       "1  娱乐  ６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...\n",
       "2  娱乐  超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...\n",
       "3  娱乐  吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...\n",
       "4  娱乐  组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_csv('.\\sohudata\\data\\sohu_train.txt',sep='\\t', header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df  = train_df.dropna()\n",
    "#train_data = train_df.content.values.tolist()  # 长度是24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 \t 2000\n",
      "健康 \t 2000\n",
      "女人 \t 2000\n",
      "娱乐 \t 2000\n",
      "房地产 \t 2000\n",
      "教育 \t 2000\n",
      "文化 \t 2000\n",
      "新闻 \t 2000\n",
      "旅游 \t 2000\n",
      "汽车 \t 2000\n",
      "科技 \t 2000\n",
      "财经 \t 2000\n"
     ]
    }
   ],
   "source": [
    "## 查看train 每个分类的名字和样本数量：\n",
    "for name,group in train_df.groupby(0):\n",
    "    print(name,\"\\t\",len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 \t 1000\n",
      "健康 \t 1000\n",
      "女人 \t 1000\n",
      "娱乐 \t 1000\n",
      "房地产 \t 1000\n",
      "教育 \t 1000\n",
      "文化 \t 1000\n",
      "新闻 \t 1000\n",
      "旅游 \t 1000\n",
      "汽车 \t 1000\n",
      "科技 \t 1000\n",
      "财经 \t 1000\n"
     ]
    }
   ],
   "source": [
    "### 同样的方法查看test 集合上各个分类名字和样本数量:\n",
    "test_df = pd.read_csv('.\\sohudata\\data\\sohu_test.txt', sep='\\t', header=None)\n",
    "for name,group in test_df.groupby(0):\n",
    "    print(name,\"\\t\",len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name,group in test_df.groupby(0):\n",
    "    pass\n",
    "    #print(name[0])\n",
    "    #print(group[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\THEONE~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.609 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章分词共花费52.91秒\n",
      "前2000篇文章分词共花费102.47秒\n",
      "前3000篇文章分词共花费207.17秒\n",
      "前4000篇文章分词共花费307.23秒\n",
      "前5000篇文章分词共花费401.17秒\n",
      "前6000篇文章分词共花费499.24秒\n",
      "前7000篇文章分词共花费526.00秒\n",
      "前8000篇文章分词共花费552.98秒\n",
      "前9000篇文章分词共花费594.73秒\n",
      "前10000篇文章分词共花费634.51秒\n",
      "前11000篇文章分词共花费680.76秒\n",
      "前12000篇文章分词共花费731.16秒\n",
      "前13000篇文章分词共花费761.25秒\n",
      "前14000篇文章分词共花费789.65秒\n",
      "前15000篇文章分词共花费828.72秒\n",
      "前16000篇文章分词共花费869.18秒\n",
      "前17000篇文章分词共花费908.40秒\n",
      "前18000篇文章分词共花费945.69秒\n",
      "前19000篇文章分词共花费1033.60秒\n",
      "前20000篇文章分词共花费1122.37秒\n",
      "前21000篇文章分词共花费1144.25秒\n",
      "前22000篇文章分词共花费1166.31秒\n",
      "前23000篇文章分词共花费1200.72秒\n",
      "前24000篇文章分词共花费1234.73秒\n"
     ]
    }
   ],
   "source": [
    "import jieba, time\n",
    "train_df.columns = ['分类', '文章']\n",
    "#stopword_list = [k.strip() for k in open('stopwords.txt', encoding='utf-8').readlines() if k.strip() != '']\n",
    "#上面的语句不建议这么写，因为readlines()是一下子将所有内容读入内存，如果文件过大，会很耗内存，建议这么写\n",
    "stopword_list = [k.strip() for k in open('.\\sohudata\\data\\stopwords.txt', encoding='utf-8') if k.strip() != '']\n",
    "cutWords_list = []\n",
    " \n",
    "i = 0\n",
    "startTime = time.time()\n",
    "for article in train_df['文章']:\n",
    "    cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print('前%d篇文章分词共花费%.2f秒' % (i, time.time() - startTime))\n",
    "    cutWords_list.append(cutWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list1.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[:2000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list2.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[2001:4000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list3.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[4001:6000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list4.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[6001:8000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list5.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[8001:10000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list6.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[10001:12000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list7.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[12001:14000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list8.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[14001:16000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('.\\sohudata\\data\\cutWords_list9.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[16001:18000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('.\\sohudata\\data\\cutWords_list10.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[18001:20000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('.\\sohudata\\data\\cutWords_list11.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[20001:22000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('.\\sohudata\\data\\cutWords_list12.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[20001:22000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('.\\sohudata\\data\\cutWords_list13.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list[22001:24000]:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 载入分词文件：\n",
    "with open('.\\sohudata\\data\\cutWords_list.txt') as file:\n",
    "    cutWords_list  = [k.split() for k in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model  = Word2Vec(cutWords_list,size = 100,window = 2,iter = 1,min_count = 20)\n",
    "# sentences 可以是一个list,建议使用BrownCorpus，Text8Corpus或lineSentence构建\n",
    "# size: 特征向量的维度，默认是100,大的size需要等多的训练数据，但是效果会更好，推荐几十到几百\n",
    "# min_count: 可以对字典进行截断,词频少于min_count次数的单词会被丢掉，默认值是5\n",
    "# iter :表示的是迭代次数\n",
    "# sg: 表示的是训练算法；sg = 0，表示的是CBOW, sg =1，表示的是skim-gram\n",
    "#alpha： 是学习率的初始化值\n",
    "#min_count: 如果单词出现的频率小于这个数的时候，就会忽略\n",
    "#hs : 等于0 的话通过负例采样来模型训练，hs = 1 表示的是通过混层softmax来模型训练\n",
    "#window：表示当前单词与预测单词之间的最大距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('主任', 0.9960504770278931),\n",
       " ('地面', 0.9959766864776611),\n",
       " ('总经理', 0.995343029499054),\n",
       " ('力度', 0.9938451051712036)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 通过Word2Vec 来找跟目标词相近的词，可以返回需要找到几个相近的词，及相似度\n",
    "word2vec_model.wv.most_similar(\"体育\",topn = 4) # 跟词语\"运动\"相似的4个词分别是：返回的是一个列表，列表里面的\n",
    "# 元素是元祖(第一个是相关的词汇，第2个是相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('韩剧', 0.9973145723342896)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### word2vec_model 方法使用postive 和negative 这2个关键词参数的简单实例，“女人 - ？ = 男人 - 先生\n",
    "word2vec_model.most_similar(positive = ['女人','先生'],negative = ['男人'],topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "word2vec_model.save('.\\sohudata\\data\\word2vec_model.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 对于每一篇文章，获取文章的每个分词在word2vec模型中的向量；然后每一篇文章中词分词都可以在word2vec模型中的\n",
    "### 相关性向量的求和后的平均数，即此篇文章在word2vec模型中的相关向量\n",
    "### 下面实例化第一个Word2Vec一个对象时，关键字参数size = 100,相关性矩阵都是100\n",
    "### 新建一个getVector 计算一篇文章的词向量，传入2个参数，1个是word分词结果，1个是word2vec 模型对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章所形成词向量花费的时间10.60\n",
      "前2000篇文章所形成词向量花费的时间20.76\n",
      "处理全部花费的时间:20.76\n"
     ]
    }
   ],
   "source": [
    "def getVector_v1(cutWords,word2vec_model):\n",
    "    count = 0\n",
    "    article_vector = np.zeros(word2vec_model.layer1_size)\n",
    "    for cutWord in cutWords:\n",
    "        if cutWord in word2vec_model:\n",
    "            article_vector+= word2vec_model[cutWord]\n",
    "            count+=1 \n",
    "    return article_vector/count # 返回的是文档中每个词对应词向量的平均值\n",
    "# 取一部分数量的词来更换成词向量\n",
    "cutword_nums = 5000\n",
    "start = time.time()\n",
    "vector_list = []\n",
    "i = 0\n",
    "for cutWords in cutWords_list[:cutword_nums]:\n",
    "    vector_list.append(getVector_v1(cutWords,word2vec_model))\n",
    "    i+=1\n",
    "    if i%1000 == 0:\n",
    "        print(\"前%d篇文章所形成词向量花费的时间%0.2f\" %(i,time.time() - start))\n",
    "X = np.array(vector_list)\n",
    "print(\"处理全部花费的时间:%0.2f\"%(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### j将计算的特征矩阵save成文档来\n",
    "X.dump('.\\sohudata\\data\\articles_vector.txt') # dump方法传入一个字符串\n",
    "X = np.load('.\\sohudata\\data\\articles_vector.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 5.模型训练及模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1）标签编码成为数字\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df = pd.read_csv('.\\sohudata\\data\\sohu_train.txt', sep='\\t', header=None)\n",
    "train_df.columns = ['label','example']\n",
    "labelencoder = LabelEncoder()\n",
    "y            = labelencoder.fit_transform(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2)引入LR回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,text_X,train_y,test_y = train_test_split(X,y,test_size = 0.2,random_state = 0.2)\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_X,train_y)\n",
    "print(\"准确率：\",logistic_model.score(text_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 通过joblib来保存model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logistic_model,'.\\sohudata\\data\\logistic.model')\n",
    "## 加载model\n",
    "logistic_model = joblib.load('.\\sohudata\\data\\logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 交叉验证\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cv_split = ShuffleSplit(n_splits= 5,train_size = 0.7,test_size = 0.2)\n",
    "log_model       = LogisticRegression()\n",
    "score_ndarray   = cross_val_score(log_model,X,y,cv = cv_split)\n",
    "print(score_ndarray)\n",
    "print(score_ndarray.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### m模型测试\n",
    "import jieba\n",
    "def getVectorMatrix(article_series):\n",
    "    return np.array([getVector_v1(jieba.cut(k),word2vec_model) for k in article_series])\n",
    "test_df = pd.read_csv('.\\sohudata\\data\\sohu_test.txt', sep='\\t', header=None)\n",
    "logistic_model = joblib.load('.\\sohudata\\data\\logistic.model')\n",
    "test_df.columns = ['分类','文章']\n",
    "for name,group in test_df.groupby('分类'):\n",
    "    featurematrix = getVectorMatrix(group['文章'])\n",
    "    target        = labelencoder.transfrom(groupby['分类'])\n",
    "    print(name,logistic_model.score(featurematrix,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 计算准确率和召回率\n",
    "from sklearn.metrics import classification_report\n",
    "test_df = pd.read_csv('.\\sohudata\\data\\sohu_test.txt', sep='\\t', header=None)\n",
    "test_df.columns = ['分类','文章']\n",
    "test_label      = labelencoder.transfrom(test_df['分类'])\n",
    "test_pred          = logistic_model.score(getVectorMatrix(test_df['文章']))\n",
    "print(labelencoder.inverse_transform([[x] for x in range(12)]))\n",
    "print(classification_report(test_label,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2大部分，通过TFidfVectorize 模型来抽取特征完成完成文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "withe open('.\\sohudata\\data\\cutWords_list.txt') as file:\n",
    "    cutWords_list = [k.split() for k in file] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###1.TFidfVectorize 模型 - 通过sklearn.feature_extraction import tfidfVectorize 实现文本特征的抽取\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(cutWords_list,stop_words =stopword_list,min_df = 40,max_df = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 特征工程\n",
    "train_df = pd.read_csv('.\\sohudata\\data\\sohu_train.txt', sep='\\t', header=None)\n",
    "train_df.columns = ['分类','文章']\n",
    "X = tfidf.fit_transform(train_df['文章'])\n",
    "print(\"词表大小:\",len(tfidf.vocabulary_))\n",
    "print(X.shape)  # 可见每篇文章内容被向量化，维度特征值是: 3946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 训练数据 - 标签 \n",
    "from sklearn.preprocssing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "import pandas as pd\n",
    "y        = labelencoder.fit_transform(train_df[0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,text_X,train_y,test_y = train_test_split(X,y,test_size = 0.2,random_state = 0.2)\n",
    "log_clf = LogisticRegression(multi_class = 'multinomial',solver = 'lbfgs')\n",
    "log_clf.fit(train_X,train_y)\n",
    "log_clf.score(text_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save模型\n",
    "import pickle\n",
    "with open('.\\sohudata\\data\\tfidf.model','wb') as file:\n",
    "    save = {\n",
    "        'lableEncoder': labelencoder,\n",
    "        'tfidfvectorizer':tfidf,\n",
    "        'log_model':log_clf\n",
    "    }\n",
    "pickle.dump(save,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 交叉验证\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cv_split        = ShuffleSplit(n_splits= 5,test_size = 0.3)\n",
    "log_model       = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "score_ndarray   = cross_val_score(log_model,X,y,cv = cv_split)\n",
    "print(score_ndarray)\n",
    "print(score_ndarray.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##模型评估 - 混淆矩阵\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "logistic_model = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs')\n",
    "logistic_model.fit(train_X, train_y)\n",
    "predict_y = logistic_model.predict(test_X)\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y),columns=labelEncoder.classes_, index=labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 绘制precision、recall、f1-score、support报告表：\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    " \n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    #计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support( y_true, y_pred)\n",
    "    #计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision' : p,\n",
    "        u'Recall' : r,\n",
    "        u'F1' : f1,\n",
    "        u'Support' : s\n",
    "    })\n",
    "     \n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label' : ['总体'],\n",
    "        u'Precision' : [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1' : [tot_f1],\n",
    "        u'Support' : [tot_s]\n",
    "    })\n",
    "     \n",
    "    res2.index = [999]\n",
    "    res = pd.concat( [res1, res2])\n",
    "    return res[ ['Label', 'Precision', 'Recall', 'F1', 'Support'] ]\n",
    " \n",
    "predict_y = logistic_model.predict(test_X)\n",
    "eval_model(test_y, predict_y, labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 模型测试\n",
    "import pandas as pd\n",
    " \n",
    "test_df = pd.read_csv('sohu_test.txt', sep='\\t', header=None)\n",
    "test_X = tfidfVectorizer.transform(test_df[1])\n",
    "test_y = labelEncoder.transform(test_df[0])\n",
    "predict_y = logistic_model.predict(test_X)\n",
    " \n",
    "eval_model(test_y, predict_y, labelEncoder.classes_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
