{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 下面代码来之：https://www.cnblogs.com/always-fight/p/10159547.html\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0  娱乐  《青蛇》造型师默认新《红楼梦》额妆抄袭（图） 凡是看过电影《青蛇》的人，都不会忘记青白二蛇的...\n",
       "1  娱乐  ６．１６日剧榜　＜最后的朋友＞　亮最后杀招成功登顶 《最后的朋友》本周的电视剧排行榜单依然只...\n",
       "2  娱乐  超乎想象的好看《纳尼亚传奇２：凯斯宾王子》 现时资讯如此发达，搜狐电影评审团几乎人人在没有看...\n",
       "3  娱乐  吴宇森：赤壁大战不会出现在上集 “希望《赤壁》能给你们不一样的感觉。”对于自己刚刚拍完的影片...\n",
       "4  娱乐  组图：《多情女人痴情男》陈浩民现场耍宝 陈浩民：外面的朋友大家好，现在是搜狐现场直播，欢迎《..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('.\\sohudata\\data\\sohu_train.txt',sep='\\t', header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df  = train_df.dropna()\n",
    "#train_data = train_df.content.values.tolist()  # 长度是24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 \t 2000\n",
      "健康 \t 2000\n",
      "女人 \t 2000\n",
      "娱乐 \t 2000\n",
      "房地产 \t 2000\n",
      "教育 \t 2000\n",
      "文化 \t 2000\n",
      "新闻 \t 2000\n",
      "旅游 \t 2000\n",
      "汽车 \t 2000\n",
      "科技 \t 2000\n",
      "财经 \t 2000\n"
     ]
    }
   ],
   "source": [
    "## 查看train 每个分类的名字和样本数量：\n",
    "for name,group in train_df.groupby(0):\n",
    "    print(name,\"\\t\",len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育 \t 1000\n",
      "健康 \t 1000\n",
      "女人 \t 1000\n",
      "娱乐 \t 1000\n",
      "房地产 \t 1000\n",
      "教育 \t 1000\n",
      "文化 \t 1000\n",
      "新闻 \t 1000\n",
      "旅游 \t 1000\n",
      "汽车 \t 1000\n",
      "科技 \t 1000\n",
      "财经 \t 1000\n"
     ]
    }
   ],
   "source": [
    "### 同样的方法查看test 集合上各个分类名字和样本数量:\n",
    "test_df = pd.read_csv('.\\sohudata\\data\\sohu_test.txt', sep='\\t', header=None)\n",
    "for name,group in test_df.groupby(0):\n",
    "    print(name,\"\\t\",len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name,group in test_df.groupby(0):\n",
    "    pass\n",
    "    #print(name[0])\n",
    "    #print(group[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\THEONE~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.609 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章分词共花费52.91秒\n",
      "前2000篇文章分词共花费102.47秒\n",
      "前3000篇文章分词共花费207.17秒\n",
      "前4000篇文章分词共花费307.23秒\n",
      "前5000篇文章分词共花费401.17秒\n",
      "前6000篇文章分词共花费499.24秒\n",
      "前7000篇文章分词共花费526.00秒\n",
      "前8000篇文章分词共花费552.98秒\n",
      "前9000篇文章分词共花费594.73秒\n",
      "前10000篇文章分词共花费634.51秒\n",
      "前11000篇文章分词共花费680.76秒\n",
      "前12000篇文章分词共花费731.16秒\n",
      "前13000篇文章分词共花费761.25秒\n",
      "前14000篇文章分词共花费789.65秒\n",
      "前15000篇文章分词共花费828.72秒\n",
      "前16000篇文章分词共花费869.18秒\n",
      "前17000篇文章分词共花费908.40秒\n",
      "前18000篇文章分词共花费945.69秒\n",
      "前19000篇文章分词共花费1033.60秒\n",
      "前20000篇文章分词共花费1122.37秒\n",
      "前21000篇文章分词共花费1144.25秒\n",
      "前22000篇文章分词共花费1166.31秒\n",
      "前23000篇文章分词共花费1200.72秒\n",
      "前24000篇文章分词共花费1234.73秒\n"
     ]
    }
   ],
   "source": [
    "import jieba, time\n",
    "train_df.columns = ['分类', '文章']\n",
    "#stopword_list = [k.strip() for k in open('stopwords.txt', encoding='utf-8').readlines() if k.strip() != '']\n",
    "#上面的语句不建议这么写，因为readlines()是一下子将所有内容读入内存，如果文件过大，会很耗内存，建议这么写\n",
    "stopword_list = [k.strip() for k in open('.\\sohudata\\data\\stopwords.txt', encoding='utf-8') if k.strip() != '']\n",
    "cutWords_list = []\n",
    " \n",
    "i = 0\n",
    "startTime = time.time()\n",
    "for article in train_df['文章']:\n",
    "    cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print('前%d篇文章分词共花费%.2f秒' % (i, time.time() - startTime))\n",
    "    cutWords_list.append(cutWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 将分词后的结果保存在.txt文件里面\n",
    "with open('.\\sohudata\\data\\cutWords_list.txt','w',encoding = 'utf-8') as file:\n",
    "    for cutWords in cutWords_list:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 载入分词文件：\n",
    "cutWords_list = []\n",
    "with open('./sohudata/data/cutWords_list.txt','r',encoding = 'utf-8') as file:\n",
    "    cutWords_list  = [k.split() for k in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f\"%4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练24000个样本花费的时间是 52.07 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "start = time.time()\n",
    "word2vec_model  = Word2Vec(cutWords_list,size = 100,window = 2,iter = 200,min_count = 20)\n",
    "end = time.time()\n",
    "print(\"训练%d个样本花费的时间是 %0.2f\"%(len(cutWords_list),(end - start)/60),'mins')\n",
    "# sentences 可以是一个list,建议使用BrownCorpus，Text8Corpus或lineSentence构建\n",
    "# size: 特征向量的维度，默认是100,大的size需要等多的训练数据，但是效果会更好，推荐几十到几百\n",
    "# min_count: 可以对字典进行截断,词频少于min_count次数的单词会被丢掉，默认值是5\n",
    "# iter :表示的是迭代次数\n",
    "# sg: 表示的是训练算法；sg = 0，表示的是CBOW, sg =1，表示的是skim-gram\n",
    "#alpha： 是学习率的初始化值\n",
    "#min_count: 如果单词出现的频率小于这个数的时候，就会忽略\n",
    "#hs : 等于0 的话通过负例采样来模型训练，hs = 1 表示的是通过混层softmax来模型训练\n",
    "#window：表示当前单词与预测单词之间的最大距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('高兴', 0.6634989976882935),\n",
       " ('自豪', 0.6148637533187866),\n",
       " ('欣慰', 0.5899534225463867),\n",
       " ('快乐', 0.5826623439788818),\n",
       " ('愉快', 0.5618224143981934),\n",
       " ('生气', 0.5613652467727661),\n",
       " ('感动', 0.5603732466697693),\n",
       " ('心情', 0.5381132364273071),\n",
       " ('对不起', 0.5371996164321899),\n",
       " ('兴奋', 0.523831307888031)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 通过Word2Vec 来找跟目标词相近的词，可以返回需要找到几个相近的词，及相似度\n",
    "word2vec_model.wv.most_similar(\"开心\",topn =10) # 跟词语\"运动\"相似的4个词分别是：返回的是一个列表，列表里面的\n",
    "# 元素是元祖(第一个是相关的词汇，第2个是相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('女士', 0.7172855138778687)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### word2vec_model 方法使用postive 和negative 这2个关键词参数的简单实例，“女人 - ？ = 男人 - 先生\n",
    "word2vec_model.most_similar(positive = ['女人','先生'],negative = ['男人'],topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "word2vec_model.save('./sohudata/data/word2vec_model.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1重新加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('高兴', 0.6634989976882935),\n",
       " ('自豪', 0.6148637533187866),\n",
       " ('欣慰', 0.5899534225463867),\n",
       " ('快乐', 0.5826623439788818)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec.load('./sohudata/data/word2vec_model.w2v')\n",
    "word2vec_model.wv.most_similar('开心',topn= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 对于每一篇文章，获取文章的每个分词在word2vec模型中的向量；然后每一篇文章中词分词都可以在word2vec模型中的\n",
    "### 相关性向量的求和后的平均数，即此篇文章在word2vec模型中的相关向量\n",
    "### 下面实例化第一个Word2Vec一个对象时，关键字参数size = 100,相关性矩阵都是100\n",
    "### 新建一个getVector 计算一篇文章的词向量，传入2个参数，1个是word分词结果，1个是word2vec 模型对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getVector_v1(cutWords,word2vec_model):\n",
    "    count = 0\n",
    "    article_vector = np.zeros(word2vec_model.layer1_size)\n",
    "    for cutWord in cutWords:\n",
    "        if cutWord in word2vec_model:\n",
    "            article_vector+= word2vec_model[cutWord]\n",
    "            count+=1 \n",
    "    return article_vector/count # 返回的是文档中每个词对应词向量的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n",
      "  app.launch_new_instance()\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前1000篇文章所形成词向量花费的时间11.08 sec\n",
      "前2000篇文章所形成词向量花费的时间21.79 sec\n"
     ]
    }
   ],
   "source": [
    "# 取一部分数量的词来更换成词向量\n",
    "import time\n",
    "import numpy as np\n",
    "cutword_nums = 24000\n",
    "start = time.time()\n",
    "vector_list = []\n",
    "i = 0\n",
    "for cutWords in cutWords_list[:cutword_nums]:\n",
    "    vector_list.append(getVector_v1(cutWords,word2vec_model))\n",
    "    i+=1\n",
    "    if i%1000 == 0:\n",
    "        print(\"前%d篇文章所形成词向量花费的时间%0.2f\" %(i,time.time() - start),'sec')\n",
    "X = np.array(vector_list)\n",
    "print(\"处理全部花费的时间:%0.2f\"%(time.time() - start),'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.6957985 ,  0.53137462,  1.04064428, -0.2385629 ,  0.12440227,\n",
       "         0.16879624, -0.14543796,  0.05181002, -0.18992081,  0.32969225,\n",
       "         0.09313782,  0.25629261,  0.1265765 , -0.66114717, -0.325841  ,\n",
       "        -0.13366155, -1.12290873, -0.00698046, -0.09335969,  0.43577012,\n",
       "         0.55911621, -1.06186359, -0.1593309 ,  0.48744267,  0.19711087,\n",
       "        -0.43444411, -0.72344278,  0.29048578, -0.21027578, -0.07362615,\n",
       "         0.49724962,  0.1162595 ,  0.04271776, -0.42054305, -0.33989077,\n",
       "         0.89218481, -0.27475017,  0.10951941, -0.24385046,  0.05179802,\n",
       "         0.8296283 ,  0.1879013 , -0.38489069, -0.35859662,  0.17663303,\n",
       "         0.58216028, -0.05308534, -0.30152697,  0.74189372,  0.60776241,\n",
       "         0.52278352,  0.2623954 , -0.57246163,  0.67103611,  0.27365632,\n",
       "        -0.34883797,  1.00951477,  0.74290665, -0.42941052,  0.64676393,\n",
       "         0.88888832,  0.18519186, -0.11065338, -0.0743731 , -0.42954011,\n",
       "         0.70717746, -0.24690223, -0.01141937,  0.62953278, -0.05354117,\n",
       "        -0.38695486,  0.07290633, -0.56107989,  0.30344851, -0.06993726,\n",
       "        -0.08502321,  0.44987138, -0.34585919,  0.59760884, -0.08813179,\n",
       "        -0.17178189, -0.13260427,  0.69771787,  0.12999652, -0.28376693,\n",
       "         0.19393096, -0.26489233,  0.15051665,  0.41583504, -0.12725538,\n",
       "        -0.69531708, -0.26017789,  0.26698818,  0.29536678, -0.15714162,\n",
       "         0.16919241, -0.26513534, -0.04370205,  0.79301428,  0.1564379 ]), 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "dump not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dd9a7f7f73cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m### j将计算的特征矩阵save成文档来\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/articles_vector.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dump方法传入一个字符串\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: dump not found"
     ]
    }
   ],
   "source": [
    "### j将计算的特征矩阵save成文档来\n",
    "X.dump('./sohudata/data/articles_vector.txt') # dump方法传入一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.load('./sohudata/data/articles_vector.txt',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.模型训练及模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1）标签编码成为数字\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "train_df         = pd.read_csv('./sohudata/data/sohu_train.txt', sep='\\t', header=None)\n",
    "train_df.columns = ['label','example']\n",
    "labelencoder = LabelEncoder()\n",
    "y            = labelencoder.fit_transform(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, '娱乐')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[22],train_df['label'][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：77.12 %\n"
     ]
    }
   ],
   "source": [
    "### 2)引入LR回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_X,text_X,train_y,test_y = train_test_split(X,y,test_size = 0.2,random_state = 200)\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_X,train_y)\n",
    "print(\"准确率：%0.2f\"%(logistic_model.score(text_X,test_y)*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 通过joblib来保存model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logistic_model,'./sohudata/data/logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sohudata/data/logistic.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c6b6aefa59f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m## 加载model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogistic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/logistic.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sohudata/data/logistic.model'"
     ]
    }
   ],
   "source": [
    "## 加载model\n",
    "from sklearn.externals import joblib\n",
    "logistic_model = joblib.load('./sohudata/data/logistic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78083333 0.78145833 0.78770833 0.791875   0.77875    0.77770833\n",
      " 0.78354167 0.78041667 0.791875   0.784375  ]\n",
      "0.7838541666666667\n"
     ]
    }
   ],
   "source": [
    "### 交叉验证\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cv_split = ShuffleSplit(len(X),train_size = 0.7,test_size = 0.2)\n",
    "log_model       = LogisticRegression()\n",
    "score_ndarray   = cross_val_score(log_model,X,y,cv = cv_split)\n",
    "print(score_ndarray)\n",
    "print(score_ndarray.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator LogisticRegression from version pre-0.18 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: The file './sohudata/data/logistic.model' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getVector_v1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f76db0c2aa85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'分类'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'文章'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'分类'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfeaturematrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetVectorMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'文章'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarget\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mlabelencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'分类'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogistic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturematrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f76db0c2aa85>\u001b[0m in \u001b[0;36mgetVectorMatrix\u001b[0;34m(article_series)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetVectorMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgetVector_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_series\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/sohu_test.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogistic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/logistic.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-f76db0c2aa85>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetVectorMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgetVector_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2vec_model\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_series\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/sohu_test.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogistic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./sohudata/data/logistic.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getVector_v1' is not defined"
     ]
    }
   ],
   "source": [
    "### m模型测试\n",
    "import jieba\n",
    "def getVectorMatrix(article_series):\n",
    "    return np.array([getVector_v1(jieba.cut(k),word2vec_model) for k in article_series])\n",
    "test_df = pd.read_csv('./sohudata/data/sohu_test.txt', sep='\\t', header=None)\n",
    "logistic_model = joblib.load('./sohudata/data/logistic.model')\n",
    "test_df.columns = ['分类','文章']\n",
    "for name,group in test_df.groupby('分类'):\n",
    "    featurematrix = getVectorMatrix(group['文章'])\n",
    "    target        = labelencoder.transform(group['分类'])\n",
    "    print(name,logistic_model.score(featurematrix,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getVectorMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fbec6c69e5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'分类'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'文章'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_label\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mlabelencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'分类'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_pred\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetVectorMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'文章'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getVectorMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "### 计算准确率和召回率\n",
    "from sklearn.metrics import classification_report\n",
    "test_df = pd.read_csv('./sohudata/data/sohu_test.txt', sep='\\t', header=None)\n",
    "test_df.columns = ['分类','文章']\n",
    "test_label      = labelencoder.transform(test_df['分类'])\n",
    "test_pred       = logistic_model.score(getVectorMatrix(test_df['文章']),test_label)\n",
    "print(labelencoder.inverse_transform([[x] for x in range(12)]))\n",
    "print(classification_report(test_label,test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##项目分割线###--------------------------------------------###项目分割线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2大部分，通过TFidfVectorize 模型来抽取特征完成完成文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutWords_list = []\n",
    "with open('./sohudata/data/cutWords_list.txt','r',encoding = 'utf-8') as file:\n",
    "    cutWords_list = [k.split() for k in file] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###1.TFidfVectorize 模型 - 通过sklearn.feature_extraction import tfidfVectorize 实现文本特征的抽取\n",
    "stopword_list = [k.strip() for k in open('.\\sohudata\\data\\stopwords.txt', encoding='utf-8') if k.strip() != '']\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(cutWords_list,stop_words =stopword_list,min_df = 40,max_df = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 3946\n",
      "(24000, 3946)\n"
     ]
    }
   ],
   "source": [
    "### 特征工程\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('./sohudata/data/sohu_train.txt', sep='\\t', header=None)\n",
    "train_df.columns = ['分类','文章']\n",
    "X = tfidf.fit_transform(train_df['文章'])\n",
    "print(\"词表大小:\",len(tfidf.vocabulary_))\n",
    "print(X.shape)  # 可见每篇文章内容被向量化，维度特征值是: 3946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 训练数据 - 标签 \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "train_df = pd.read_csv('.\\sohudata\\data\\sohu_train.txt',sep='\\t', header=None)\n",
    "train_df.head()\n",
    "import pandas as pd\n",
    "y        = labelencoder.fit_transform(train_df[0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率是86.71 %\n"
     ]
    }
   ],
   "source": [
    "### 逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_X,text_X,train_y,test_y = train_test_split(X,y,test_size = 0.2,random_state = 200)\n",
    "log_clf = LogisticRegression(multi_class = 'multinomial',solver = 'lbfgs')\n",
    "log_clf.fit(train_X,train_y)\n",
    "print(\"准确率是%0.2f\"%(100*log_clf.score(text_X,test_y)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save 模型\n",
    "import pickle\n",
    "file = open('./sohudata/data/tfidf.model','wb')\n",
    "save_data = {\n",
    "    'labelencoder':labelencoder,\n",
    "    'tfidf':tfidf,\n",
    "    'log_clf':log_clf\n",
    "}\n",
    "pickle.dump(save_data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 重新加载模型\n",
    "import pickle\n",
    "with open('./sohudata/data/tfidf.model', 'rb') as file:  #读取2进制文件\n",
    "    tfidf_model = pickle.load(file)\n",
    "    tfidfVectorizer = tfidf_model['tfidf']\n",
    "    labelEncoder = tfidf_model['labelencoder']\n",
    "    logistic_model = tfidf_model['log_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重新获得特征和标签值\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('./sohudata/data/sohu_train.txt', sep='\\t', header=None)\n",
    "X = tfidfVectorizer.transform(train_df[1])\n",
    "y = labelEncoder.transform(train_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.875       0.87625     0.87458333  0.8775      0.87416667  0.87958333\n",
      "  0.87416667  0.86625     0.87354167  0.86958333]\n",
      "交叉验证的准确率是87.41 %\n"
     ]
    }
   ],
   "source": [
    "### 交叉验证\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cv_split        = ShuffleSplit(X.shape[0],train_size = 0.7,test_size = 0.2) # X.shape[0] = 24000\n",
    "log_model       = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "score_ndarray   = cross_val_score(log_model,X,y,cv = cv_split)\n",
    "print(score_ndarray)\n",
    "print(\"交叉验证的准确率是%0.2f\"%(score_ndarray.mean()*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>体育</th>\n",
       "      <th>健康</th>\n",
       "      <th>女人</th>\n",
       "      <th>娱乐</th>\n",
       "      <th>房地产</th>\n",
       "      <th>教育</th>\n",
       "      <th>文化</th>\n",
       "      <th>新闻</th>\n",
       "      <th>旅游</th>\n",
       "      <th>汽车</th>\n",
       "      <th>科技</th>\n",
       "      <th>财经</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>体育</th>\n",
       "      <td>405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>健康</th>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>女人</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>349</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>娱乐</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>361</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>房地产</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>365</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>教育</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文化</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>338</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>新闻</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>331</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>旅游</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>361</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>汽车</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>340</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>科技</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>383</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>财经</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      体育   健康   女人   娱乐  房地产   教育   文化   新闻   旅游   汽车   科技   财经\n",
       "体育   405    0    0    2    5    2    6    6    0    2    4    2\n",
       "健康     0  384    5    2    2    1    2    3    2    3    9    2\n",
       "女人     2    3  349    5    3    2    4    1    1    3    0    2\n",
       "娱乐     5    1    2  361    1    1    5    2    2    2    1    0\n",
       "房地产    2    3    0    2  365    2   11   21    3    2    6   10\n",
       "教育     2    4    3    2    4  357    2    2    2    5    2    3\n",
       "文化     5    2    6   10    6    3  338    6    4    3    4    0\n",
       "新闻     4    1    1    2   12    3    5  331    2    2    5    9\n",
       "旅游     3    2    2    4    5    4    8    8  361    2    3    3\n",
       "汽车     3    5    1    2    5    8    6    2    5  340    2    5\n",
       "科技     2    1    1    1   11    2    6    7    4    2  383    5\n",
       "财经     1    2    0    1   14    3    2   16    2    5    8  346"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##模型评估 - 混淆矩阵\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "logistic_model = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs')\n",
    "logistic_model.fit(train_X, train_y)\n",
    "predict_y = logistic_model.predict(test_X)\n",
    "pd.DataFrame(confusion_matrix(test_y, predict_y),columns=labelEncoder.classes_, index=labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.959077</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.935996</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>健康</td>\n",
       "      <td>0.919802</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.924378</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女人</td>\n",
       "      <td>0.936874</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.935936</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.925595</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.929283</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>房地产</td>\n",
       "      <td>0.831361</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.837140</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.912475</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>文化</td>\n",
       "      <td>0.841193</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.857283</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>新闻</td>\n",
       "      <td>0.814643</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.845599</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>旅游</td>\n",
       "      <td>0.941922</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.916281</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>汽车</td>\n",
       "      <td>0.920945</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.908815</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.891019</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.886991</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.864024</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.896831</td>\n",
       "      <td>0.895583</td>\n",
       "      <td>0.895955</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Precision    Recall        F1  Support\n",
       "0     体育   0.959077  0.914000  0.935996     1000\n",
       "1     健康   0.919802  0.929000  0.924378     1000\n",
       "2     女人   0.936874  0.935000  0.935936     1000\n",
       "3     娱乐   0.925595  0.933000  0.929283     1000\n",
       "4    房地产   0.831361  0.843000  0.837140     1000\n",
       "5     教育   0.912475  0.907000  0.909729     1000\n",
       "6     文化   0.841193  0.874000  0.857283     1000\n",
       "7     新闻   0.814643  0.879000  0.845599     1000\n",
       "8     旅游   0.941922  0.892000  0.916281     1000\n",
       "9     汽车   0.920945  0.897000  0.908815     1000\n",
       "10    科技   0.891019  0.883000  0.886991     1000\n",
       "11    财经   0.867069  0.861000  0.864024     1000\n",
       "12    总体   0.896831  0.895583  0.895955    12000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 绘制precision、recall、f1-score、support报告表：\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    " \n",
    "def eval_model(y_true, y_pred, labels):\n",
    "    #计算每个分类的Precision, Recall, f1, support\n",
    "    p, r, f1, s = precision_recall_fscore_support( y_true, y_pred)\n",
    "    #计算总体的平均Precision, Recall, f1, support\n",
    "    tot_p = np.average(p, weights=s)\n",
    "    tot_r = np.average(r, weights=s)\n",
    "    tot_f1 = np.average(f1, weights=s)\n",
    "    tot_s = np.sum(s)\n",
    "    res1 = pd.DataFrame({\n",
    "        u'Label': labels,\n",
    "        u'Precision' : p,\n",
    "        u'Recall' : r,\n",
    "        u'F1' : f1,\n",
    "        u'Support' : s\n",
    "    })\n",
    "     \n",
    "    res2 = pd.DataFrame({\n",
    "        u'Label' : ['总体'],\n",
    "        u'Precision' : [tot_p],\n",
    "        u'Recall': [tot_r],\n",
    "        u'F1' : [tot_f1],\n",
    "        u'Support' : [tot_s]\n",
    "    })\n",
    "     \n",
    "    res2.index = [12]\n",
    "    res = pd.concat( [res1, res2])\n",
    "    return res[ ['Label', 'Precision', 'Recall', 'F1', 'Support'] ]\n",
    " \n",
    "predict_y = logistic_model.predict(test_X)\n",
    "eval_model(test_y, predict_y, labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>体育</td>\n",
       "      <td>0.959077</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.935996</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>健康</td>\n",
       "      <td>0.919802</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.924378</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女人</td>\n",
       "      <td>0.936874</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.935936</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>娱乐</td>\n",
       "      <td>0.925595</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>0.929283</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>房地产</td>\n",
       "      <td>0.831361</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.837140</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>教育</td>\n",
       "      <td>0.912475</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.909729</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>文化</td>\n",
       "      <td>0.841193</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.857283</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>新闻</td>\n",
       "      <td>0.814643</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.845599</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>旅游</td>\n",
       "      <td>0.941922</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.916281</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>汽车</td>\n",
       "      <td>0.920945</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.908815</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>科技</td>\n",
       "      <td>0.891019</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.886991</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>财经</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.864024</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>总体</td>\n",
       "      <td>0.896831</td>\n",
       "      <td>0.895583</td>\n",
       "      <td>0.895955</td>\n",
       "      <td>12000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Precision    Recall        F1  Support\n",
       "0     体育   0.959077  0.914000  0.935996     1000\n",
       "1     健康   0.919802  0.929000  0.924378     1000\n",
       "2     女人   0.936874  0.935000  0.935936     1000\n",
       "3     娱乐   0.925595  0.933000  0.929283     1000\n",
       "4    房地产   0.831361  0.843000  0.837140     1000\n",
       "5     教育   0.912475  0.907000  0.909729     1000\n",
       "6     文化   0.841193  0.874000  0.857283     1000\n",
       "7     新闻   0.814643  0.879000  0.845599     1000\n",
       "8     旅游   0.941922  0.892000  0.916281     1000\n",
       "9     汽车   0.920945  0.897000  0.908815     1000\n",
       "10    科技   0.891019  0.883000  0.886991     1000\n",
       "11    财经   0.867069  0.861000  0.864024     1000\n",
       "12    总体   0.896831  0.895583  0.895955    12000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 模型测试\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('./sohudata/data/sohu_test.txt', sep='\\t', header=None)\n",
    "test_X = tfidfVectorizer.transform(test_df[1])\n",
    "test_y = labelEncoder.transform(test_df[0])\n",
    "predict_y = logistic_model.predict(test_X)\n",
    "\n",
    "eval_model(test_y, predict_y, labelEncoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
