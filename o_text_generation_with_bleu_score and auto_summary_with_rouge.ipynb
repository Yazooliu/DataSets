{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 关于Rouge[自动摘要]的理解可以参考：https://www.cnblogs.com/yuguangchuan/p/4092728.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Rouge是从召回率的角度出发： \n",
    "## 分子：Countmatch(n-gram)表示同时出现在机器摘要和参考摘要中的n-gram的个数\n",
    "## 分母是：Count(n-gram) 表示参考摘要中的n-gram的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------\n",
    "# 2019-11-12 \n",
    "# text generation and auto summary metric definations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于BLEU[文本生成]的函数理解可以参考：https://blog.csdn.net/CharlesOyfz/article/details/90668423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BELU 是从准确率的角度出发：通过引入n-gram 来计算比值：分母是模型预测出来的文本的n-gram组成的集合，用数###量来表示；分子是既出现在预测中的n-gram\n",
    "### 又出现在参考摘要的中的n-gram数量\n",
    "### BLEU = (出现在预测摘要同时又出现在参考摘要中的n-gram 的数量)/(模型预测出来样本的n-gram数量) (n一般小于4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu(reference, candidate, weight=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    利用nltk工具包计算bleu值\n",
    "    :param reference: 二维数组\n",
    "    :param candidate: 一维数组\n",
    "    :param weight: 从1-4gram的权重\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 定义平滑函数对象\n",
    "    smooth = SmoothingFunction()\n",
    "    score  = sentence_bleu(reference, candidate, weights=weight, smoothing_function=smooth.method1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu(true_y, pred_y, weight=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    计算batch数据的bleu值\n",
    "    :param true_y: 真实值\n",
    "    :param pred_y: 预测值\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bleus = []\n",
    "    for i in range(len(true_y)):\n",
    "        score = bleu([true_y[i]], pred_y[i], weight)\n",
    "        bleus.append(score)\n",
    "    return mean(bleus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "               'ensures', 'that', 'the', 'military', 'always',\n",
    "               'obeys', 'the', 'commands', 'of', 'the', 'party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "              'ensures', 'that', 'the', 'military', 'will', 'forever', \n",
    "              'heed', 'Party', 'commands']\n",
    "reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "              'guarantees', 'the', 'military', 'forces', 'always',\n",
    "              'being', 'under', 'the', 'command', 'of', 'the',\n",
    "              'Party']\n",
    "reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "              'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "              'of', 'the', 'party']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重Weight决定最高可由几个grams (n =1,2,3,4,5,6) 来做BLEU的评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = (0.1666, 0.1666, 0.1666, 0.1666, 0.1666) # 1/6 = 6-grams 模型\n",
    "weights = (1/3,1/3,1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reference1 = ['The','cat','is','on','the','mat']\n",
    "reference2 =  ['There','is','a','cat','on','the','mat']\n",
    "hypothesis1 = ['the','cat','the','cat','on','the','mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = SmoothingFunction()\n",
    "smoothing_function = smooth.method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.563123940221803"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([reference1,reference2], hypothesis1, weights,smoothing_function = smoothing_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SmoothingFunction in module nltk.translate.bleu_score:\n",
      "\n",
      "class SmoothingFunction(builtins.object)\n",
      " |  This is an implementation of the smoothing techniques \n",
      " |  for segment-level BLEU scores that was presented in \n",
      " |  Boxing Chen and Collin Cherry (2014) A Systematic Comparison of \n",
      " |  Smoothing Techniques for Sentence-Level BLEU. In WMT14. \n",
      " |  http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, epsilon=0.1, alpha=5, k=5)\n",
      " |      This will initialize the parameters required for the various smoothing\n",
      " |      techniques, the default values are set to the numbers used in the\n",
      " |      experiments from Chen and Cherry (2014).\n",
      " |      \n",
      " |      >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', \n",
      " |      ...                 'that', 'the', 'military', 'always', 'obeys', 'the', \n",
      " |      ...                 'commands', 'of', 'the', 'party']\n",
      " |      >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', \n",
      " |      ...               'that', 'the', 'military', 'will', 'forever', 'heed', \n",
      " |      ...               'Party', 'commands']\n",
      " |              \n",
      " |      >>> chencherry = SmoothingFunction()\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1)) # doctest: +ELLIPSIS\n",
      " |      0.4118...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method0)) # doctest: +ELLIPSIS\n",
      " |      0.4118...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method1)) # doctest: +ELLIPSIS\n",
      " |      0.4118...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method2)) # doctest: +ELLIPSIS\n",
      " |      0.4489...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method3)) # doctest: +ELLIPSIS\n",
      " |      0.4118...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method4)) # doctest: +ELLIPSIS\n",
      " |      0.4118...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method5)) # doctest: +ELLIPSIS\n",
      " |      0.4905...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method6)) # doctest: +ELLIPSIS\n",
      " |      0.1801...\n",
      " |      >>> print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method7)) # doctest: +ELLIPSIS\n",
      " |      0.4905...\n",
      " |      \n",
      " |      :param epsilon: the epsilon value use in method 1\n",
      " |      :type epsilon: float\n",
      " |      :param alpha: the alpha value use in method 6\n",
      " |      :type alpha: int\n",
      " |      :param k: the k value use in method 4\n",
      " |      :type k: int\n",
      " |  \n",
      " |  method0(self, p_n, *args, **kwargs)\n",
      " |      No smoothing.\n",
      " |  \n",
      " |  method1(self, p_n, *args, **kwargs)\n",
      " |      Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n",
      " |  \n",
      " |  method2(self, p_n, *args, **kwargs)\n",
      " |      Smoothing method 2: Add 1 to both numerator and denominator from \n",
      " |      Chin-Yew Lin and Franz Josef Och (2004) Automatic evaluation of \n",
      " |      machine translation quality using longest common subsequence and \n",
      " |      skip-bigram statistics. In ACL04.\n",
      " |  \n",
      " |  method3(self, p_n, *args, **kwargs)\n",
      " |      Smoothing method 3: NIST geometric sequence smoothing \n",
      " |      The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each \n",
      " |      precision score whose matching n-gram count is null.\n",
      " |      k is 1 for the first 'n' value for which the n-gram match count is null/\n",
      " |      For example, if the text contains:\n",
      " |       - one 2-gram match\n",
      " |       - and (consequently) two 1-gram matches\n",
      " |      the n-gram count for each individual precision score would be:\n",
      " |       - n=1  =>  prec_count = 2     (two unigrams)\n",
      " |       - n=2  =>  prec_count = 1     (one bigram)\n",
      " |       - n=3  =>  prec_count = 1/2   (no trigram,  taking 'smoothed' value of 1 / ( 2^k ), with k=1)\n",
      " |       - n=4  =>  prec_count = 1/4   (no fourgram, taking 'smoothed' value of 1 / ( 2^k ), with k=2)\n",
      " |  \n",
      " |  method4(self, p_n, references, hypothesis, hyp_len)\n",
      " |      Smoothing method 4: \n",
      " |      Shorter translations may have inflated precision values due to having \n",
      " |      smaller denominators; therefore, we give them proportionally\n",
      " |      smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry \n",
      " |      suggests dividing by 1/ln(len(T)), where T is the length of the translation.\n",
      " |  \n",
      " |  method5(self, p_n, references, hypothesis, hyp_len)\n",
      " |      Smoothing method 5:\n",
      " |      The matched counts for similar values of n should be similar. To a \n",
      " |      calculate the n-gram matched count, it averages the n−1, n and n+1 gram \n",
      " |      matched counts.\n",
      " |  \n",
      " |  method6(self, p_n, references, hypothesis, hyp_len)\n",
      " |      Smoothing method 6:\n",
      " |      Interpolates the maximum likelihood estimate of the precision *p_n* with \n",
      " |      a prior estimate *pi0*. The prior is estimated by assuming that the ratio \n",
      " |      between pn and pn−1 will be the same as that between pn−1 and pn−2.\n",
      " |  \n",
      " |  method7(self, p_n, references, hypothesis, hyp_len)\n",
      " |      Smoothing method 6:\n",
      " |      Interpolates the maximum likelihood estimate of the precision *p_n* with \n",
      " |      a prior estimate *pi0*. The prior is estimated by assuming that the ratio \n",
      " |      between pn and pn−1 will be the same as that between pn−1 and pn−2.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SmoothingFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sentence_bleu in module nltk.translate.bleu_score:\n",
      "\n",
      "sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None)\n",
      "    Calculate BLEU score (Bilingual Evaluation Understudy) from\n",
      "    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n",
      "    \"BLEU: a method for automatic evaluation of machine translation.\" \n",
      "    In Proceedings of ACL. http://www.aclweb.org/anthology/P02-1040.pdf\n",
      "    \n",
      "    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
      "    ...               'ensures', 'that', 'the', 'military', 'always',\n",
      "    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
      "    \n",
      "    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
      "    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
      "    ...               'that', 'party', 'direct']\n",
      "    \n",
      "    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
      "    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
      "    ...               'heed', 'Party', 'commands']\n",
      "    \n",
      "    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
      "    ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
      "    ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
      "    ...               'Party']\n",
      "    \n",
      "    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
      "    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
      "    ...               'of', 'the', 'party']\n",
      "    \n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\n",
      "    0.5045...\n",
      "    \n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2) # doctest: +ELLIPSIS\n",
      "    0.3969...\n",
      "    \n",
      "    The default BLEU calculates a score for up to 4grams using uniform\n",
      "    weights. To evaluate your translations with higher/lower order ngrams, \n",
      "    use customized weights. E.g. when accounting for up to 6grams with uniform\n",
      "    weights:\n",
      "    \n",
      "    >>> weights = (0.1666, 0.1666, 0.1666, 0.1666, 0.1666)\n",
      "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)\n",
      "    0.45838627164939455\n",
      "    \n",
      "    :param references: reference sentences\n",
      "    :type references: list(list(str))\n",
      "    :param hypothesis: a hypothesis sentence\n",
      "    :type hypothesis: list(str)\n",
      "    :param weights: weights for unigrams, bigrams, trigrams and so on\n",
      "    :type weights: list(float)\n",
      "    :return: The sentence-level BLEU score.\n",
      "    :rtype: float\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sentence_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "reference = [['The','cat','is','on','the','mat']]\n",
    "candidate = ['The','cat','sat','on','the','mat']\n",
    "smooth = SmoothingFunction()  # 定义平滑函数对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5946035575013605"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights= (1/4,1/4,1/4)\n",
    "sentence_bleu(reference, candidate, weights,smooth.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
