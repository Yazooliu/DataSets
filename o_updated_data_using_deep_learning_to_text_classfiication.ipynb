{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##　准备数据\n",
    "import jieba\n",
    "import pandas as pd \n",
    "\n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna() # 取消可能为空的行数\n",
    "# sprots\n",
    "df_sprots   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_society\n",
    "df_society = pd.read_csv(\"./data/society_news.csv\",encoding = 'utf-8')\n",
    "df_society = df_society.dropna()\n",
    "\n",
    "#military\n",
    "df_military   = pd.read_csv(\"./data/military_news.csv\", encoding = 'utf-8')\n",
    "df_military   = df_military.dropna()\n",
    "\n",
    "df_internality = pd.read_csv(\"./data/international_news.csv\",encoding = 'utf-8')\n",
    "df_international = df_internality.dropna()\n",
    "\n",
    "df_house = pd.read_csv(\"./data/house_news.csv\",encoding = 'utf-8')\n",
    "df_house = df_house.dropna()\n",
    "\n",
    "df_home = pd.read_csv(\"./data/home_news.csv\",encoding = 'utf-8')\n",
    "df_home = df_home.dropna()\n",
    "\n",
    "df_finance = pd.read_csv(\"./data/finance_news.csv\",encoding = 'utf-8')\n",
    "df_finance = df_finance.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每个种类提取一定数量的文本样本记录\n",
    "technology     = df_technology.content.values.tolist()\n",
    "car            = df_car.content.values.tolist()\n",
    "sprots         = df_sprots.content.values.tolist()\n",
    "military       = df_military.content.values.tolist()\n",
    "society        = df_society.content.values.tolist()\n",
    "international  = df_international.content.values.tolist()\n",
    "house          = df_house.content.values.tolist()\n",
    "home           = df_home.content.values.tolist()\n",
    "finance        = df_finance.content.values.tolist()\n",
    "entertainment  = df_entertainment.content.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 额外的添加一些数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 从其他地方借来的数据 - from sohu data \n",
    "with open('./sohudata/data/sohu_train_car.txt','r',encoding= 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        car.append(line)\n",
    "        \n",
    "with open('./sohudata/data/sohu_train_house.txt','r',encoding= 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        house.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 从其他地方借来的数据 - from cnew.train.data\n",
    "with open(\"./cnews/cnews_train_sport.txt\",'r',encoding= 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        sprots.append(line)\n",
    "        \n",
    "with open(\"./cnews/cnews_train_tech.txt\",'r',encoding= 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        technology.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./cnews/cnews_train_house.txt\",'r',encoding= 'utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        house.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1统计文本长度 -  文本分类原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20064, 24826, 191272, 72353, 11342, 115116, 9186, 113065, 28039, 11849]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllTexts        = [technology,sprots,society,international,house,home,car,finance,entertainment,military]\n",
    "AllTexts_length = []\n",
    "lens = []\n",
    "for text in AllTexts:\n",
    "    count = 0\n",
    "    for oneExample in text:\n",
    "        if len(oneExample)>=50:\n",
    "            count = count +1\n",
    "            lens.append(len(oneExample))\n",
    "    AllTexts_length.append(count)\n",
    "#AllTexts_length.sort()\n",
    "AllTexts_length # 样本长度比较短的是 [military,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label1S      = [5066, 4449, 26795, 4807, 5113, 21534, 5312, 19927, 8508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label1       = ['科技','体育', '社会', '国际', '房产', '家居', '汽车', '财经','娱乐'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label2_sohu  = ['体育', '健康', '女人', '娱乐', '房地产', '教育', '文化', '新闻', '旅游', '汽车', '科技', '财经']  #3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label3_cnews = ['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经'] #5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1.1统计文本长度 -  sohu数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file   = \"./cnews/cnews_train_house.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writor = open(file,'w',encoding = 'utf-8')\n",
    "with open('./cnews/cnews.train.txt','r',encoding = 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        assert(len(line.split('\\t')) == 2)\n",
    "        items = line.split('\\t')\n",
    "        label = items[0]\n",
    "        content = items[1]\n",
    "        if label == '房产':\n",
    "            writor.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 923.50413846153845, 27471, 65000, 2773.3759383087481, 43.150769230769228)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filetrain = './cnews/cnews.train.txt'\n",
    "filetest = './cnews/cnews.test.txt'\n",
    "fileval = './cnews/cnews.val.txt'\n",
    "files = [filetrain,filetest,fileval]\n",
    "lens = []\n",
    "for file in files:\n",
    "    with open(file,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            lens.append(len(line))\n",
    "            \n",
    "import numpy as np\n",
    "lenss = np.array(lens)\n",
    "np.min(lenss),np.mean(lenss),np.max(lenss),len(lenss), np.mean(lenss) + 2*np.std(lenss),np.sum(lenss<600)/len(lenss)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 554.35139784946239, 36997, 55800, 3443.6430721524366, 86.702508960573482)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filetrain = './newdata/random/cnews.train.txt'\n",
    "filetest = './newdata/random/cnews.test.txt'\n",
    "fileval = './newdata/random/cnews.val.txt'\n",
    "files = [filetrain,filetest,fileval]\n",
    "lens = []\n",
    "for file in files:\n",
    "    with open(file,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            lens.append(len(line))\n",
    "            \n",
    "import numpy as np\n",
    "lenss = np.array(lens)\n",
    "np.min(lenss),np.mean(lenss),np.max(lenss),len(lenss), np.mean(lenss) + 2*np.std(lenss),np.sum(lenss<350)/len(lenss)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2606.3519047619047, 88189, 37800, 7965.4533806715181, 5.0238095238095237)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filetrain = './sohudata/data/sohu_train.txt'\n",
    "filetest =  './sohudata/data/sohu_test.txt'\n",
    "files = [filetrain,filetest,fileval]\n",
    "lens = []\n",
    "for file in files:\n",
    "    with open(file,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            lens.append(len(line))\n",
    "            \n",
    "import numpy as np\n",
    "lenss = np.array(lens)\n",
    "np.min(lenss),np.mean(lenss),np.max(lenss),len(lenss), np.mean(lenss) + 2*np.std(lenss),np.sum(lenss<400)/len(lenss)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2手动切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcontents = []\n",
    "def Train_Test_Validation_Data_Split(filetrain,filetest,fileval,contents,labels,limits,lens):\n",
    "    data_train = open(filetrain, 'a', encoding='utf-8')\n",
    "    data_test  = open(filetest, 'a', encoding='utf-8')\n",
    "    data_val   = open(fileval, 'a', encoding='utf-8')\n",
    "    count = 0\n",
    "    for content in contents:\n",
    "        if len(content)>=limits:\n",
    "            content = content.strip()\n",
    "            count = count + 1\n",
    "            if count<=lens:\n",
    "                data_train.write(labels+'\\t'+content+'\\n')\n",
    "            elif lens<count and count<=lens+1000:\n",
    "                data_test.write(labels+'\\t'+content+'\\n')\n",
    "            elif lens+1000<count and count<lens+1201:\n",
    "                data_val.write(labels+'\\t'+content+'\\n')\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits = 145  #抽取超过特定长度的文本 \n",
    "lens  =  5000 #每个样本抽取6000个样本\n",
    "#AllTexts = [technology,sprots,society,military,international,house,home,car,finance,entertainment]\n",
    "#labels   = ['技术','体育','社会','军事','国际','房产','家庭','汽车','金融','娱乐']\n",
    "AllTexts = [international,house,home,car,finance,entertainment,technology,society,sprots]\n",
    "labels   = ['国际','房产','家庭','汽车','金融', '娱乐','技术','社会','体育']\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "for i in range(0,len(AllTexts)):\n",
    "    Train_Test_Validation_Data_Split(filetrain,filetest,fileval,AllTexts[i],labels[i],limits,lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3判断文件中的一行是不是为空，为空直接删掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_blank(file1,file2):\n",
    "    file1 = open(file1, 'r', encoding='utf-8') # 要去掉空行的文件 \n",
    "    file2 = open(file2, 'w', encoding='utf-8') # 生成没有空行的文件\n",
    "    try:\n",
    "        for line in file1.readlines():\n",
    "            if line == '\\n':\n",
    "                line = line.strip(\"\\n\")\n",
    "            file2.write(line)\n",
    "    finally:\n",
    "        file1.close()\n",
    "        file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "\n",
    "# 清理空行后的数据\n",
    "filetrain1 = './newdata/cnews.train.txt'\n",
    "filetest1  = './newdata/cnews.test.txt'\n",
    "fileval1   = './newdata/cnews.val.txt'\n",
    "\n",
    "del_blank(filetrain,filetrain1)\n",
    "del_blank(filetest,filetest1)\n",
    "del_blank(fileval,fileval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4判断文件中的一行是不是为空，为空直接删掉；同时将样本乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def delandrandom(file1,file2):\n",
    "    contents = []\n",
    "    file1 = open(file1, 'r', encoding='utf-8') # 要去掉空行的文件 \n",
    "    file2 = open(file2, 'w', encoding='utf-8') # 生成没有空行的文件\n",
    "    try:\n",
    "        for line in file1.readlines():\n",
    "            if line == '\\n':\n",
    "                line = line.strip(\"\\n\") #如果是空行直接删掉空行\n",
    "                file2.write(line)\n",
    "            else:\n",
    "                contents.append(line)\n",
    "            \n",
    "        \n",
    "        random.shuffle(contents)\n",
    "        for content in contents:\n",
    "            file2.write(content)\n",
    "        \n",
    "    finally:\n",
    "        file1.close()\n",
    "        file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "\n",
    "# 清理空行后的数据\n",
    "filetrain1 = './newdata/random/cnews.train.txt'\n",
    "filetest1  = './newdata/random/cnews.test.txt'\n",
    "fileval1   = './newdata/random/cnews.val.txt'\n",
    "\n",
    "delandrandom(filetrain,filetrain1)\n",
    "delandrandom(filetest,filetest1)\n",
    "delandrandom(fileval,fileval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于样本收集词汇表和定制词嵌入矩阵###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import tensorflow.contrib.keras as kr\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def read_file(filename):\n",
    "    re_han = re.compile(u\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)\")  # the method of cutting text by punctuation\n",
    "    contents, labels = [], []\n",
    "    with codecs.open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = line.rstrip()\n",
    "                assert len(line.split('\\t')) == 2\n",
    "                label, content = line.split('\\t')\n",
    "                labels.append(label)\n",
    "                blocks = re_han.split(content)\n",
    "                word = []\n",
    "                for blk in blocks:\n",
    "                    if re_han.match(blk):\n",
    "                        word.extend(jieba.lcut(blk))\n",
    "                contents.append(word)\n",
    "            except:\n",
    "                pass\n",
    "    return labels, contents\n",
    "# labels[322] = '体育'\n",
    "# np.array(contents[322]) = ['罗伊吃', '消炎药', '对抗', '膝伤', '开拓者',......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def built_vocab_vector(filenames,stopword_path,vector_word_path,vocab_word_path,vector_word_npz_file,voc_size=10000):\n",
    "    '''\n",
    "    去停用词，得到前9999个词，获取对应的词 以及 词向量\n",
    "    :param filenames:\n",
    "    :param voc_size:\n",
    "    :return:\n",
    "    '''\n",
    "    stopword = open(stopword_path, 'r', encoding='utf-8')\n",
    "    stop = [key.strip(' \\n') for key in stopword]\n",
    "    all_data = []\n",
    "    j = 1\n",
    "    embeddings = np.zeros([10000, 100])  # 9656 是有效样本长度大小，仅仅需要更改这个即可，不许更改voc_size\n",
    "    for filename in filenames:\n",
    "        labels, content = read_file(filename)\n",
    "        for eachline in content:\n",
    "            line =[]\n",
    "            for i in range(len(eachline)):\n",
    "                if str(eachline[i]) not in stop:#去停用词\n",
    "                    line.append(eachline[i])\n",
    "            all_data.extend(line)\n",
    "    counter = Counter(all_data)\n",
    "    count_paris = counter.most_common(voc_size-1)\n",
    "    word, _ = list(zip(*count_paris))\n",
    "    f = codecs.open(vector_word_path, 'r', encoding='utf-8')\n",
    "    vocab_word = open(vocab_word_path, 'w', encoding='utf-8')\n",
    "    for ealine in f:\n",
    "        item = ealine.split(' ')\n",
    "        key = item[0]\n",
    "        vec = np.array(item[1:], dtype='float32')\n",
    "        if key in word:\n",
    "            embeddings[j] = np.array(vec)\n",
    "            vocab_word.write(key.strip('\\r') + '\\n')\n",
    "            j += 1\n",
    "    \n",
    "    print(\"embeddings 填充行数是:\",j)\n",
    "    np.savez_compressed(vector_word_npz_file, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理空行后的数据\n",
    "train_filename = './newdata/random/cnews.train.txt'\n",
    "test_filename  = './newdata/random/cnews.test.txt'\n",
    "val_filename   = './newdata/random/cnews.val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理空行后的数据\n",
    "train_filename = './cnews/cnews.train.txt'\n",
    "test_filename  = './cnews/cnews.test.txt'\n",
    "val_filename   = './cnews/cnews.val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [train_filename,test_filename,val_filename] # input is one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_path    = './cnews/stopwords.txt'\n",
    "vector_word_path = './cnews/vector_word.txt'\n",
    "vocab_word_path  =  './cnews/vocab_word.txt'\n",
    "vector_word_npz_file = './cnews/vector_word.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "built_vocab_vector() got multiple values for argument 'voc_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-ce0cb2205d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbuilt_vocab_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstopword_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector_word_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_word_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector_word_npz_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvoc_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"运行时间%0.2f:\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mins'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: built_vocab_vector() got multiple values for argument 'voc_size'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "built_vocab_vector(filenames,stopword_path,vector_word_path,vocab_word_path,vector_word_npz_file)\n",
    "t2 = time.time()\n",
    "print(\"运行时间%0.2f:\"%((t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
