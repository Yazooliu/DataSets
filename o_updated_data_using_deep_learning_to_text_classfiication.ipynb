{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##　准备数据\n",
    "import jieba\n",
    "import pandas as pd \n",
    "\n",
    "# df_technology\n",
    "df_technology  = pd.read_csv(\"./data/technology_news.csv\", encoding = 'utf-8')\n",
    "df_technology  = df_technology.dropna() # 取消可能为空的行数\n",
    "# sprots\n",
    "df_sprots   = pd.read_csv(\"./data/sports_news.csv\", encoding = 'utf-8')\n",
    "df_sprots   = df_sprots.dropna()\n",
    "\n",
    "#df_society\n",
    "df_society = pd.read_csv(\"./data/society_news.csv\",encoding = 'utf-8')\n",
    "df_society = df_society.dropna()\n",
    "\n",
    "#military\n",
    "df_military   = pd.read_csv(\"./data/military_news.csv\", encoding = 'utf-8')\n",
    "df_military   = df_military.dropna()\n",
    "\n",
    "df_internality = pd.read_csv(\"./data/international_news.csv\",encoding = 'utf-8')\n",
    "df_international = df_internality.dropna()\n",
    "\n",
    "df_house = pd.read_csv(\"./data/house_news.csv\",encoding = 'utf-8')\n",
    "df_house = df_house.dropna()\n",
    "\n",
    "df_home = pd.read_csv(\"./data/home_news.csv\",encoding = 'utf-8')\n",
    "df_home = df_home.dropna()\n",
    "\n",
    "df_finance = pd.read_csv(\"./data/finance_news.csv\",encoding = 'utf-8')\n",
    "df_finance = df_finance.dropna()\n",
    "\n",
    "# df_entertainment \n",
    "df_entertainment   = pd.read_csv(\"./data/entertainment_news.csv\", encoding = 'utf-8')\n",
    "df_entertainment   = df_entertainment.dropna()\n",
    "\n",
    "# df_car\n",
    "df_car  = pd.read_csv(\"./data/car_news.csv\", encoding = 'utf-8')\n",
    "df_car  = df_car.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每个种类提取一定数量的文本样本记录\n",
    "technology     = df_technology.content.values.tolist()\n",
    "car            = df_car.content.values.tolist()\n",
    "sprots         = df_sprots.content.values.tolist()\n",
    "military       = df_military.content.values.tolist()\n",
    "society        = df_society.content.values.tolist()\n",
    "international  = df_international.content.values.tolist()\n",
    "house          = df_house.content.values.tolist()\n",
    "home           = df_home.content.values.tolist()\n",
    "finance        = df_finance.content.values.tolist()\n",
    "entertainment  = df_entertainment.content.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1统计文本长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13807, 15266, 104606, 6508, 26947, 7096, 66151, 6282, 68893, 20021]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllTexts = [technology,sprots,society,military,international,house,home,car,finance,entertainment]\n",
    "AllTexts_length = []\n",
    "for text in AllTexts:\n",
    "    count = 0\n",
    "    for oneExample in text:\n",
    "        if len(oneExample)>=100:\n",
    "            count = count +1\n",
    "    AllTexts_length.append(count)\n",
    "#AllTexts_length.sort()\n",
    "AllTexts_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300.0, 2610.0, 90.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9000*0.7,9000*0.29,9000*0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2手动切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcontents = []\n",
    "def Train_Test_Validation_Data_Split(filetrain,filetest,fileval,contents,labels,limits,lens):\n",
    "    data_train = open(filetrain, 'a', encoding='utf-8')\n",
    "    data_test  = open(filetest, 'a', encoding='utf-8')\n",
    "    data_val   = open(fileval, 'a', encoding='utf-8')\n",
    "    count = 0\n",
    "    for content in contents:\n",
    "        if len(content)>=limits:\n",
    "            count = count + 1\n",
    "            if count<lens:\n",
    "                data_train.write(labels+'\\t'+content+'\\n')\n",
    "            elif lens<=count and count<lens*1.1:\n",
    "                data_test.write(labels+'\\t'+content+'\\n')\n",
    "            elif lens*1.1<=count and count<lens*1.148:\n",
    "                data_val.write(labels+'\\t'+content+'\\n')\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits = 50  #抽取超过特定长度的文本 \n",
    "lens  =  8000 #每个样本抽取8000个样本\n",
    "#AllTexts = [technology,sprots,society,military,international,house,home,car,finance,entertainment]\n",
    "#labels   = ['技术','体育','社会','军事','国际','房产','家庭','汽车','金融','娱乐']\n",
    "AllTexts = [military,international,house,home,car,finance,entertainment,technology,society,sprots]\n",
    "labels   = ['军事','国际','房产','家庭','汽车','金融','娱乐','技术','社会','体育']\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "for i in range(0,len(AllTexts)):\n",
    "    Train_Test_Validation_Data_Split(filetrain,filetest,fileval,AllTexts[i],labels[i],limits,lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3判断文件中的一行是不是为空，为空直接删掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_blank(file1,file2):\n",
    "    file1 = open(file1, 'r', encoding='utf-8') # 要去掉空行的文件 \n",
    "    file2 = open(file2, 'w', encoding='utf-8') # 生成没有空行的文件\n",
    "    try:\n",
    "        for line in file1.readlines():\n",
    "            if line == '\\n':\n",
    "                line = line.strip(\"\\n\")\n",
    "            file2.write(line)\n",
    "    finally:\n",
    "        file1.close()\n",
    "        file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "\n",
    "# 清理空行后的数据\n",
    "filetrain1 = './newdata/cnews.train.txt'\n",
    "filetest1  = './newdata/cnews.test.txt'\n",
    "fileval1   = './newdata/cnews.val.txt'\n",
    "\n",
    "del_blank(filetrain,filetrain1)\n",
    "del_blank(filetest,filetest1)\n",
    "del_blank(fileval,fileval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4判断文件中的一行是不是为空，为空直接删掉；同时将样本乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def delandrandom(file1,file2):\n",
    "    contents = []\n",
    "    file1 = open(file1, 'r', encoding='utf-8') # 要去掉空行的文件 \n",
    "    file2 = open(file2, 'w', encoding='utf-8') # 生成没有空行的文件\n",
    "    try:\n",
    "        for line in file1.readlines():\n",
    "            if line == '\\n':\n",
    "                line = line.strip(\"\\n\") #如果是空行直接删掉空行\n",
    "                file2.write(line)\n",
    "            else:\n",
    "                contents.append(line)\n",
    "            \n",
    "        \n",
    "        random.shuffle(contents)\n",
    "        for content in contents:\n",
    "            file2.write(content)\n",
    "        \n",
    "    finally:\n",
    "        file1.close()\n",
    "        file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "filetrain = './newdata/cnews_train.txt'\n",
    "filetest  = './newdata/cnews_test.txt'\n",
    "fileval   = './newdata/cnews_val.txt'\n",
    "\n",
    "# 清理空行后的数据\n",
    "filetrain1 = './newdata/random/cnews.train.txt'\n",
    "filetest1  = './newdata/random/cnews.test.txt'\n",
    "fileval1   = './newdata/random/cnews.val.txt'\n",
    "\n",
    "delandrandom(filetrain,filetrain1)\n",
    "delandrandom(filetest,filetest1)\n",
    "delandrandom(fileval,fileval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于样本收集词汇表和定制词嵌入矩阵###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import tensorflow.contrib.keras as kr\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def read_file(filename):\n",
    "    re_han = re.compile(u\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)\")  # the method of cutting text by punctuation\n",
    "    contents, labels = [], []\n",
    "    with codecs.open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = line.rstrip()\n",
    "                assert len(line.split('\\t')) == 2\n",
    "                label, content = line.split('\\t')\n",
    "                labels.append(label)\n",
    "                blocks = re_han.split(content)\n",
    "                word = []\n",
    "                for blk in blocks:\n",
    "                    if re_han.match(blk):\n",
    "                        word.extend(jieba.lcut(blk))\n",
    "                contents.append(word)\n",
    "            except:\n",
    "                pass\n",
    "    return labels, contents\n",
    "# labels[322] = '体育'\n",
    "# np.array(contents[322]) = ['罗伊吃', '消炎药', '对抗', '膝伤', '开拓者',......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def built_vocab_vector(filenames,voc_size=12000):\n",
    "    '''\n",
    "    去停用词，得到前9999个词，获取对应的词 以及 词向量\n",
    "    :param filenames:\n",
    "    :param voc_size:\n",
    "    :return:\n",
    "    '''\n",
    "    stopword = open('./data/stopwords_NLP.txt', 'r', encoding='utf-8')\n",
    "    stop = [key.strip(' \\n') for key in stopword]\n",
    "    all_data = []\n",
    "    j = 1\n",
    "    embeddings = np.zeros([11761, 100])  # 9656 是有效样本长度大小，仅仅需要更改这个即可，不许更改voc_size\n",
    "    for filename in filenames:\n",
    "        labels, content = read_file(filename)\n",
    "        for eachline in content:\n",
    "            line =[]\n",
    "            for i in range(len(eachline)):\n",
    "                if str(eachline[i]) not in stop:#去停用词\n",
    "                    line.append(eachline[i])\n",
    "            all_data.extend(line)\n",
    "    counter = Counter(all_data)\n",
    "    count_paris = counter.most_common(voc_size-1)\n",
    "    word, _ = list(zip(*count_paris))\n",
    "    f = codecs.open('./newdata/vector_word.txt', 'r', encoding='utf-8')\n",
    "    vocab_word = open('./newdata/vocab_word.txt', 'w', encoding='utf-8')\n",
    "    for ealine in f:\n",
    "        item = ealine.split(' ')\n",
    "        key = item[0]\n",
    "        vec = np.array(item[1:], dtype='float32')\n",
    "        if key in word:\n",
    "            embeddings[j] = np.array(vec)\n",
    "            vocab_word.write(key.strip('\\r') + '\\n')\n",
    "            j += 1\n",
    "    \n",
    "    print(\"embeddings 填充行数是:\",j)\n",
    "    np.savez_compressed('./newdata/vector_word.npz', embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理空行后的数据\n",
    "train_filename = './newdata/cnews.train.txt'\n",
    "test_filename  = './newdata/cnews.test.txt'\n",
    "val_filename   = './newdata/cnews.val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = [train_filename,test_filename,val_filename] # input is one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings 填充行数是: 11761\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "built_vocab_vector(filename)\n",
    "t2 = time.time()\n",
    "print(\"运行时间%0.2f:\"%((t2 - t1)/60),'mins')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
