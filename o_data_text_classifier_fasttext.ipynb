{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code comes from https://blog.csdn.net/john_bh/article/details/79268850\n",
    "## 2019-10-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding= utf-8\n",
    "import pandas as pd\n",
    "import random\n",
    "import fasttext\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立label标签\n",
    "cate_dic = {'technology': 1, 'sports': 2, 'society': 3, 'military': 4, 'international': 5,'house':6,'home':7,'finance':8,\n",
    "            'entertainment':9,'car':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "     #利用pandas把数据读进来\n",
    "    df_technology = pd.read_csv(\"./data/technology_news.csv\",encoding =\"utf-8\")\n",
    "    df_technology=df_technology.dropna()    #去空行处理\n",
    "\n",
    "    # sprots\n",
    "    df_sprots   = pd.read_csv(\"./NLPData/sports_news.csv\", encoding = 'utf-8')\n",
    "    df_sprots   = df_sprots.dropna()\n",
    "\n",
    "    #df_society\n",
    "    df_society = pd.read_csv(\"./NLPData/society_news.csv\",encoding = 'utf-8')\n",
    "    df_society = df_society.dropna()\n",
    "    \n",
    "    df_military = pd.read_csv(\"./data/military_news.csv\",encoding =\"utf-8\")\n",
    "    df_military=df_military.dropna()\n",
    "\n",
    "    df_internality = pd.read_csv(\"./NLPData/international_news.csv\",encoding = 'utf-8')\n",
    "    df_international = df_internality.dropna()\n",
    "    \n",
    "    df_house = pd.read_csv(\"./NLPData/house_news.csv\",encoding = 'utf-8')\n",
    "    df_house = df_house.dropna()\n",
    "    \n",
    "    df_home = pd.read_csv(\"./NLPData/home_news.csv\",encoding = 'utf-8')\n",
    "    df_home = df_home.dropna()\n",
    "    \n",
    "    df_finance = pd.read_csv(\"./NLPData/finance_news.csv\",encoding = 'utf-8')\n",
    "    df_finance = df_finance.dropna()\n",
    "    \n",
    "    df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\",encoding =\"utf-8\")\n",
    "    df_entertainment=df_entertainment.dropna()\n",
    "    \n",
    "    df_car = pd.read_csv(\"./data/car_news.csv\",encoding =\"utf-8\")\n",
    "    df_car=df_car.dropna()\n",
    "\n",
    "\n",
    "    technology=df_technology.content.values.tolist()\n",
    "    sports    =df_sprots.content.values.tolist()\n",
    "    society   =df_society.content.values.tolist()\n",
    "    military  =df_military.content.values.tolist()\n",
    "    internality =df_internality.content.values.tolist()\n",
    "    house       =df_house.content.values.tolist()\n",
    "    home        =df_home.content.values.tolist()\n",
    "    finance     =df_finance.content.values.tolist()\n",
    "    entertainment=df_entertainment.content.values.tolist()\n",
    "    car          =df_car.content.values.tolist()\n",
    "   \n",
    "    return technology,sports,society,military,internality,house,home,finance,entertainment,car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.建立Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWords(datapath):\n",
    "    stopwords=pd.read_csv(datapath,index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "    stopwords=stopwords[\"stopword\"].values\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWords(datapath):\n",
    "    stopword_list = [k.strip() for k in open(datapath,encoding ='utf-8') if k.strip()!= '']\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.文本的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits= 40\n",
    "maxs  = 20000\n",
    "def preprocess_text(content_line,sentences,category,stopwords,i=0):\n",
    "    j = 0\n",
    "    for line in content_line:\n",
    "        j+=1\n",
    "        try:\n",
    "            if len(line)>limits:\n",
    "                i+=1\n",
    "                if i<=maxs:\n",
    "                    segs=jieba.lcut(line)    #利用结巴分词进行中文分词\n",
    "                    segs=filter(lambda x:len(x)>1,segs)    #去掉长度小于1的词\n",
    "                    segs=filter(lambda x:x not in stopwords,segs)    #去掉停用词\n",
    "                    sentences.append(\"__label__\"+str(category)+\" , \"+\" \".join(segs))    #把当前的文本和对应的类别拼接起来，组合成fasttext的文本格式\n",
    "                else:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print ('line is',line,'j = ',j)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.将处理好的写入到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeData(sentences,fileName):\n",
    "    print(\"writing data to fasttext format...\")\n",
    "    writor= open(fileName,'w',encoding = 'utf-8')\n",
    "    for sentence in sentences:\n",
    "        writor.write(sentence+\"\\n\")\n",
    "    print(\"Writing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessData(stopwords,saveDataFile):\n",
    "    technology,sports,society,military,internality,house,home,finance,entertainment,car = loadData()    \n",
    "\n",
    "    #去停用词，生成数据集\n",
    "    sentences=[]\n",
    "    preprocess_text(technology,sentences,cate_dic[\"technology\"],stopwords)\n",
    "    preprocess_text(sports,sentences,cate_dic[\"sports\"],stopwords)\n",
    "    preprocess_text(society,sentences,cate_dic[\"society\"],stopwords)\n",
    "    #preprocess_text(military,sentences,cate_dic[\"military\"],stopwords)\n",
    "    preprocess_text(internality,sentences,cate_dic[\"international\"],stopwords)\n",
    "    #preprocess_text(house,sentences,cate_dic[\"house\"],stopwords)\n",
    "    preprocess_text(home,sentences,cate_dic[\"home\"],stopwords)\n",
    "    preprocess_text(finance,sentences,cate_dic[\"finance\"],stopwords)\n",
    "    preprocess_text(entertainment,sentences,cate_dic[\"entertainment\"],stopwords)\n",
    "    #preprocess_text(car,sentences,cate_dic[\"car\"],stopwords)\n",
    "\n",
    "    random.shuffle(sentences)    #做乱序处理，使得同类别的样本不至于扎堆\n",
    "    writeData(sentences,saveDataFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.主函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 获取训练集合样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line is nan j =  5073\n",
      "line is nan j =  5117\n",
      "line is nan j =  6145\n",
      "line is nan j =  6148\n",
      "line is nan j =  6696\n",
      "line is nan j =  6705\n",
      "line is nan j =  6804\n",
      "line is nan j =  6886\n",
      "line is nan j =  7352\n",
      "line is nan j =  7358\n",
      "line is nan j =  13244\n",
      "line is nan j =  13710\n",
      "line is nan j =  13940\n",
      "line is nan j =  14658\n",
      "line is nan j =  15614\n",
      "line is nan j =  19869\n",
      "line is nan j =  20009\n",
      "line is nan j =  20013\n",
      "line is nan j =  20035\n",
      "line is nan j =  22104\n",
      "line is nan j =  22239\n",
      "line is nan j =  22429\n",
      "line is nan j =  23008\n",
      "line is nan j =  23009\n",
      "line is nan j =  23021\n",
      "line is nan j =  23102\n",
      "line is nan j =  23139\n",
      "line is nan j =  23164\n",
      "line is nan j =  23193\n",
      "writing data to fasttext format...\n",
      "Writing Done!\n",
      "运行时间:4.74 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "stopwordsFile    = r\"./data/stopwords_NLP.txt\"\n",
    "stopwords        = getStopWords(stopwordsFile)\n",
    "saveDataFile     = r'./data/SavedData_Fasttext.txt'\n",
    "preprocessData(stopwords,saveDataFile)\n",
    "t2 = time.time()\n",
    "print(\"运行时间:%0.2f\"%((t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 获取测试集合样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits = 40\n",
    "maxss  = maxs*0.1 #取训练数据的10%作为测试样本集合\n",
    "def preprocess_testdata(content_line,stopwords,test_data,category,i=0):\n",
    "    for line in content_line:\n",
    "        try:\n",
    "            if len(line)>limits:\n",
    "                i+=1\n",
    "                if i<=maxss:\n",
    "                    segs=jieba.lcut(line)    #利用结巴分词进行中文分词\n",
    "                    segs=filter(lambda x:len(x)>1,segs)\n",
    "                    segs=filter(lambda x:x not in stopwords,segs)  #去掉停用词\n",
    "                    test_data.append(\"__label__\"+str(category)+\" , \"+\" \".join(segs))\n",
    "                else:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print ('line is',line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessTestData(saveDataFile,stopwords):\n",
    "    technology,sports,society,military,internality,house,home,finance,entertainment,car = loadData()    \n",
    "\n",
    "    #去停用词，生成数据集\n",
    "    sentences=[]\n",
    "    preprocess_testdata(technology,stopwords,sentences,cate_dic[\"technology\"])\n",
    "    preprocess_testdata(sports,stopwords,sentences,cate_dic[\"sports\"])\n",
    "    preprocess_testdata(society,stopwords,sentences,cate_dic[\"society\"])\n",
    "    #preprocess_testdata(military,stopwords,sentences,cate_dic[\"military\"])\n",
    "    preprocess_testdata(internality,stopwords,sentences,cate_dic[\"international\"])\n",
    "    #preprocess_testdata(house,stopwords,sentences,cate_dic[\"house\"])\n",
    "    preprocess_testdata(home,stopwords,sentences,cate_dic[\"home\"])\n",
    "    preprocess_testdata(finance,stopwords,sentences,cate_dic[\"finance\"])\n",
    "    preprocess_testdata(entertainment,stopwords,sentences,cate_dic[\"entertainment\"])\n",
    "    #preprocess_testdata(car,stopwords,sentences,cate_dic[\"car\"])\n",
    "\n",
    "    random.shuffle(sentences)    #做乱序处理，使得同类别的样本不至于扎堆\n",
    "    writeData(sentences,saveDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format...\n",
      "Writing Done!\n",
      "运行时间:0.53 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "stopwordsFile    = r\"./data/stopwords_NLP.txt\"\n",
    "stopwords        = getStopWords(stopwordsFile)\n",
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "preprocessTestData(saveTestDataFile,stopwords)\n",
    "t2 = time.time()\n",
    "print(\"运行时间:%0.2f\"%((t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本长度： 140000\n",
      "测试集样本长度： 14000\n"
     ]
    }
   ],
   "source": [
    "### 计算文件中的样本个数 ### \n",
    "def get_examples(file,i=0):\n",
    "    with open(file,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            i+=1\n",
    "    return i\n",
    "print(\"训练集样本长度：\",get_examples(saveDataFile))\n",
    "print(\"测试集样本长度：\",get_examples(saveTestDataFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 fasttext做监督学习的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://fasttext.cc/blog/2019/06/25/blog-post.html#2-you-were-using-the-unofficial-fasttext-module%22\n",
    "import time \n",
    "iters = 90000\n",
    "t1 = time.time()\n",
    "saveDataFile     = r'./data/SavedData_Fasttext.txt'\n",
    "classifier  = fasttext.train_supervised(saveDataFile, lr=0.01, dim=100, epoch=iters,word_ngrams=2, loss='softmax')\n",
    "classifier.save_model(\"./data/fasttextmodel_file.bin\")\n",
    "\n",
    "print('训练%d个样本花费时间%0.2f'%(140000,(time.time() - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6*900/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01064122,  0.07548718, -0.66634226,  0.3310301 ,  0.00455639,\n",
       "        0.09786977, -0.43037549,  0.08423711,  0.28713906, -0.08672356,\n",
       "       -0.0714881 ,  0.2043383 , -0.23743363,  0.24247278,  0.45252576,\n",
       "        0.33777088,  0.05092357, -0.37443596,  0.02575258,  0.02583138,\n",
       "        0.42922723, -0.13854399, -0.34865689, -0.14165604, -0.06715799,\n",
       "        0.02642917,  0.15047339,  0.22576684, -0.27613154, -0.02834203,\n",
       "       -0.06995129,  0.07829287,  0.35598159,  0.27006507, -0.32318991,\n",
       "        0.05567789,  0.13211386,  0.05996792,  0.24034445,  0.10191624,\n",
       "        0.00989955, -0.20950255,  0.01309866,  0.36419141,  0.49048746,\n",
       "       -0.26566452,  0.30349448,  0.16712977,  0.18836325, -0.2187365 ,\n",
       "       -0.14845909, -0.30114374,  0.18026674, -0.20870715,  0.38214058,\n",
       "       -0.40106285,  0.2685633 ,  0.24539191,  0.09615939, -0.32130086,\n",
       "        0.57522315, -0.38666958,  0.18479225, -0.07552478,  0.13184352,\n",
       "        0.01038998,  0.20583837,  0.13370636, -0.04230121, -0.02758804,\n",
       "       -0.11037061,  0.33464649,  0.09619413,  0.0101646 , -0.12485212,\n",
       "        0.24870597,  0.21614659, -0.26664096,  0.17024361, -0.18902233,\n",
       "        0.03341984,  0.0746162 ,  0.06007265,  0.5335561 ,  0.14035837,\n",
       "       -0.03281526,  0.23344515,  0.05427546, -0.04915243,  0.01317536,\n",
       "        0.38560021, -0.19468071,  0.08213574, -0.11196394, -0.2870889 ,\n",
       "       -0.10766839, -0.1578718 , -0.41427359, -0.16498634,  0.1117171 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_word_vector('科技')  #获取某个词的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2重新加载模型 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.load_model(\"./data/fasttextmodel_file.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__7',\n",
       " '__label__9',\n",
       " '__label__1',\n",
       " '__label__5',\n",
       " '__label__3',\n",
       " '__label__8',\n",
       " '__label__2']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result           = classifier.test(saveTestDataFile) # 这里应该使用另外一部分不同的样本做测试\n",
    "classifier.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__5', '__label__3']], array([[ 0.61964625,  0.23291297]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 预测一个样本，返回对应的几个可能Label及对应的可能的概率，k表示几个label,threshold 控制门限大于这个门限的才显示\n",
    "text_string = ['新年 中央 新闻媒体 报道 空军 6K 飞行员 刘锐 走进 公众 视野 粉丝 军迷 眼中 驾驭 战神 男神']\n",
    "classifier.predict(text = text_string,k = 2,threshold = 0.1) # 输入是字符串 string \n",
    "#cate_dic = {'technology': 1, 'sports': 2, 'society': 3, 'military': 4, 'international': 5,'house':6,'home':7,'finance':8,\n",
    "#            'entertainment':9,'car':10}\n",
    "# 上面希望显示两个结果k=2，但是超过特定门限threshold = 0.1的只有一个结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Return the precision and recall score for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__1': {'f1score': 0.6435006435006435,\n",
       "  'precision': 0.47438330170777987,\n",
       "  'recall': 1.0},\n",
       " '__label__2': {'f1score': 0.8497981729339282,\n",
       "  'precision': 0.7388252678241596,\n",
       "  'recall': 1.0},\n",
       " '__label__3': {'f1score': 0.6003301815998799,\n",
       "  'precision': 0.4289084280506112,\n",
       "  'recall': 1.0},\n",
       " '__label__5': {'f1score': 0.8054772452678212,\n",
       "  'precision': 0.6743088334457181,\n",
       "  'recall': 1.0},\n",
       " '__label__7': {'f1score': 0.5681818181818182,\n",
       "  'precision': 0.3968253968253968,\n",
       "  'recall': 1.0},\n",
       " '__label__8': {'f1score': 0.582798833819242,\n",
       "  'precision': 0.41131687242798354,\n",
       "  'recall': 0.9995},\n",
       " '__label__9': {'f1score': 0.720620043258832,\n",
       "  'precision': 0.5634160090191658,\n",
       "  'recall': 0.9995}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "classifier.test_label(saveTestDataFile,k =2,threshold = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 返回模型的测试样本个数，准确率和召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 0.49992857142857144, 0.9998571428571429)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "classifier.test(saveTestDataFile, k=2) # 这里应该使用另外一部分不同的样本做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _FastText in module fasttext.FastText object:\n",
      "\n",
      "class _FastText(builtins.object)\n",
      " |  This class defines the API to inspect models and should not be used to\n",
      " |  create objects. It will be returned by functions such as load_model or\n",
      " |  train.\n",
      " |  \n",
      " |  In general this API assumes to be given only unicode for Python2 and the\n",
      " |  Python3 equvalent called str for any string-like arguments. All unicode\n",
      " |  strings are then encoded as UTF-8 and fed to the fastText C++ API.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __getitem__(self, word)\n",
      " |  \n",
      " |  __init__(self, model_path=None, args=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_dimension(self)\n",
      " |      Get the dimension (size) of a lookup vector (hidden layer).\n",
      " |  \n",
      " |  get_input_matrix(self)\n",
      " |      Get a copy of the full input matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_input_vector(self, ind)\n",
      " |      Given an index, get the corresponding vector of the Input Matrix.\n",
      " |  \n",
      " |  get_labels(self, include_freq=False, on_unicode_error='strict')\n",
      " |      Get the entire list of labels of the dictionary optionally\n",
      " |      including the frequency of the individual labels. Unsupervised\n",
      " |      models use words as labels, which is why get_labels\n",
      " |      will call and return get_words for this type of\n",
      " |      model.\n",
      " |  \n",
      " |  get_line(self, text, on_unicode_error='strict')\n",
      " |      Split a line of text into words and labels. Labels must start with\n",
      " |      the prefix used to create the model (__label__ by default).\n",
      " |  \n",
      " |  get_output_matrix(self)\n",
      " |      Get a copy of the full output matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_sentence_vector(self, text)\n",
      " |      Given a string, get a single vector represenation. This function\n",
      " |      assumes to be given a single line of text. We split words on\n",
      " |      whitespace (space, newline, tab, vertical tab) and the control\n",
      " |      characters carriage return, formfeed and the null character.\n",
      " |  \n",
      " |  get_subword_id(self, subword)\n",
      " |      Given a subword, return the index (within input matrix) it hashes to.\n",
      " |  \n",
      " |  get_subwords(self, word, on_unicode_error='strict')\n",
      " |      Given a word, get the subwords and their indicies.\n",
      " |  \n",
      " |  get_word_id(self, word)\n",
      " |      Given a word, get the word id within the dictionary.\n",
      " |      Returns -1 if word is not in the dictionary.\n",
      " |  \n",
      " |  get_word_vector(self, word)\n",
      " |      Get the vector representation of word.\n",
      " |  \n",
      " |  get_words(self, include_freq=False, on_unicode_error='strict')\n",
      " |      Get the entire list of words of the dictionary optionally\n",
      " |      including the frequency of the individual words. This\n",
      " |      does not include any subwords. For that please consult\n",
      " |      the function get_subwords.\n",
      " |  \n",
      " |  is_quantized(self)\n",
      " |  \n",
      " |  predict(self, text, k=1, threshold=0.0, on_unicode_error='strict')\n",
      " |      Given a string, get a list of labels and a list of\n",
      " |      corresponding probabilities. k controls the number\n",
      " |      of returned labels. A choice of 5, will return the 5\n",
      " |      most probable labels. By default this returns only\n",
      " |      the most likely label and probability. threshold filters\n",
      " |      the returned labels by a threshold on probability. A\n",
      " |      choice of 0.5 will return labels with at least 0.5\n",
      " |      probability. k and threshold will be applied together to\n",
      " |      determine the returned labels.\n",
      " |      \n",
      " |      This function assumes to be given\n",
      " |      a single line of text. We split words on whitespace (space,\n",
      " |      newline, tab, vertical tab) and the control characters carriage\n",
      " |      return, formfeed and the null character.\n",
      " |      \n",
      " |      If the model is not supervised, this function will throw a ValueError.\n",
      " |      \n",
      " |      If given a list of strings, it will return a list of results as usually\n",
      " |      received for a single line of text.\n",
      " |  \n",
      " |  quantize(self, input=None, qout=False, cutoff=0, retrain=False, epoch=None, lr=None, thread=None, verbose=None, dsub=2, qnorm=False)\n",
      " |      Quantize the model reducing the size of the model and\n",
      " |      it's memory footprint.\n",
      " |  \n",
      " |  save_model(self, path)\n",
      " |      Save the model to the given path\n",
      " |  \n",
      " |  test(self, path, k=1)\n",
      " |      Evaluate supervised model using file given by path\n",
      " |  \n",
      " |  test_label(self, path, k=1, threshold=0.0)\n",
      " |      Return the precision and recall score for each label.\n",
      " |      \n",
      " |      The returned value is a dictionary, where the key is the label.\n",
      " |      For example:\n",
      " |      f.test_label(...)\n",
      " |      {'__label__italian-cuisine' : {'precision' : 0.7, 'recall' : 0.74}}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labels\n",
      " |  \n",
      " |  words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
