{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code comes from https://blog.csdn.net/john_bh/article/details/79268850\n",
    "## 2019-10-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding= utf-8\n",
    "import pandas as pd\n",
    "import random\n",
    "import fasttext\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立label标签\n",
    "cate_dic = {'technology': 1, 'sports': 2, 'society': 3, 'military': 4, 'international': 5,'house':6,'home':7,'finance':8,\n",
    "            'entertainment':9,'car':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "     #利用pandas把数据读进来\n",
    "    df_technology = pd.read_csv(\"./data/technology_news.csv\",encoding =\"utf-8\")\n",
    "    df_technology=df_technology.dropna()    #去空行处理\n",
    "\n",
    "    # sprots\n",
    "    df_sprots   = pd.read_csv(\"./NLPData/sports_news.csv\", encoding = 'utf-8')\n",
    "    df_sprots   = df_sprots.dropna()\n",
    "\n",
    "    #df_society\n",
    "    df_society = pd.read_csv(\"./NLPData/society_news.csv\",encoding = 'utf-8')\n",
    "    df_society = df_society.dropna()\n",
    "    \n",
    "    df_military = pd.read_csv(\"./data/military_news.csv\",encoding =\"utf-8\")\n",
    "    df_military=df_military.dropna()\n",
    "\n",
    "    df_internality = pd.read_csv(\"./NLPData/international_news.csv\",encoding = 'utf-8')\n",
    "    df_international = df_internality.dropna()\n",
    "    \n",
    "    df_house = pd.read_csv(\"./NLPData/house_news.csv\",encoding = 'utf-8')\n",
    "    df_house = df_house.dropna()\n",
    "    \n",
    "    df_home = pd.read_csv(\"./NLPData/home_news.csv\",encoding = 'utf-8')\n",
    "    df_home = df_home.dropna()\n",
    "    \n",
    "    df_finance = pd.read_csv(\"./NLPData/finance_news.csv\",encoding = 'utf-8')\n",
    "    df_finance = df_finance.dropna()\n",
    "    \n",
    "    df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\",encoding =\"utf-8\")\n",
    "    df_entertainment=df_entertainment.dropna()\n",
    "    \n",
    "    df_car = pd.read_csv(\"./data/car_news.csv\",encoding =\"utf-8\")\n",
    "    df_car=df_car.dropna()\n",
    "\n",
    "\n",
    "    technology=df_technology.content.values.tolist()\n",
    "    sports    =df_sprots.content.values.tolist()\n",
    "    society   =df_society.content.values.tolist()\n",
    "    military  =df_military.content.values.tolist()\n",
    "    internality =df_internality.content.values.tolist()\n",
    "    house       =df_house.content.values.tolist()\n",
    "    home        =df_home.content.values.tolist()\n",
    "    finance     =df_finance.content.values.tolist()\n",
    "    entertainment=df_entertainment.content.values.tolist()\n",
    "    car          =df_car.content.values.tolist()\n",
    "   \n",
    "    return technology,sports,society,military,internality,house,home,finance,entertainment,car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.建立Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWords(datapath):\n",
    "    stopwords=pd.read_csv(datapath,index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "    stopwords=stopwords[\"stopword\"].values\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStopWords(datapath):\n",
    "    stopword_list = [k.strip() for k in open(datapath,encoding ='utf-8') if k.strip()!= '']\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.文本的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits= 40\n",
    "maxs  = 20000\n",
    "def preprocess_text(content_line,sentences,category,stopwords,i=0):\n",
    "    j = 0\n",
    "    for line in content_line:\n",
    "        j+=1\n",
    "        try:\n",
    "            if len(line)>limits:\n",
    "                i+=1\n",
    "                if i<=maxs:\n",
    "                    segs=jieba.lcut(line)    #利用结巴分词进行中文分词\n",
    "                    segs=filter(lambda x:len(x)>1,segs)    #去掉长度小于1的词\n",
    "                    segs=filter(lambda x:x not in stopwords,segs)    #去掉停用词\n",
    "                    sentences.append(\"__label__\"+str(category)+\" , \"+\" \".join(segs))    #把当前的文本和对应的类别拼接起来，组合成fasttext的文本格式\n",
    "                else:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print ('line is',line,'j = ',j)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.将处理好的写入到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeData(sentences,fileName):\n",
    "    print(\"writing data to fasttext format...\")\n",
    "    writor= open(fileName,'w',encoding = 'utf-8')\n",
    "    for sentence in sentences:\n",
    "        writor.write(sentence+\"\\n\")\n",
    "    print(\"Writing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessData(stopwords,saveDataFile):\n",
    "    technology,sports,society,military,internality,house,home,finance,entertainment,car = loadData()    \n",
    "\n",
    "    #去停用词，生成数据集\n",
    "    sentences=[]\n",
    "    preprocess_text(technology,sentences,cate_dic[\"technology\"],stopwords)\n",
    "    preprocess_text(sports,sentences,cate_dic[\"sports\"],stopwords)\n",
    "    preprocess_text(society,sentences,cate_dic[\"society\"],stopwords)\n",
    "    #preprocess_text(military,sentences,cate_dic[\"military\"],stopwords)\n",
    "    preprocess_text(internality,sentences,cate_dic[\"international\"],stopwords)\n",
    "    #preprocess_text(house,sentences,cate_dic[\"house\"],stopwords)\n",
    "    preprocess_text(home,sentences,cate_dic[\"home\"],stopwords)\n",
    "    preprocess_text(finance,sentences,cate_dic[\"finance\"],stopwords)\n",
    "    preprocess_text(entertainment,sentences,cate_dic[\"entertainment\"],stopwords)\n",
    "    #preprocess_text(car,sentences,cate_dic[\"car\"],stopwords)\n",
    "\n",
    "    random.shuffle(sentences)    #做乱序处理，使得同类别的样本不至于扎堆\n",
    "    writeData(sentences,saveDataFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.主函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 获取训练集合样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line is nan j =  5073\n",
      "line is nan j =  5117\n",
      "line is nan j =  6145\n",
      "line is nan j =  6148\n",
      "line is nan j =  6696\n",
      "line is nan j =  6705\n",
      "line is nan j =  6804\n",
      "line is nan j =  6886\n",
      "line is nan j =  7352\n",
      "line is nan j =  7358\n",
      "line is nan j =  13244\n",
      "line is nan j =  13710\n",
      "line is nan j =  13940\n",
      "line is nan j =  14658\n",
      "line is nan j =  15614\n",
      "line is nan j =  19869\n",
      "line is nan j =  20009\n",
      "line is nan j =  20013\n",
      "line is nan j =  20035\n",
      "line is nan j =  22104\n",
      "line is nan j =  22239\n",
      "line is nan j =  22429\n",
      "line is nan j =  23008\n",
      "line is nan j =  23009\n",
      "line is nan j =  23021\n",
      "line is nan j =  23102\n",
      "line is nan j =  23139\n",
      "line is nan j =  23164\n",
      "line is nan j =  23193\n",
      "writing data to fasttext format...\n",
      "Writing Done!\n",
      "运行时间:4.74 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "stopwordsFile    = r\"./data/stopwords_NLP.txt\"\n",
    "stopwords        = getStopWords(stopwordsFile)\n",
    "saveDataFile     = r'./data/SavedData_Fasttext.txt'\n",
    "preprocessData(stopwords,saveDataFile)\n",
    "t2 = time.time()\n",
    "print(\"运行时间:%0.2f\"%((t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 获取测试集合样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limits = 40\n",
    "maxss  = maxs*0.1 #取训练数据的10%作为测试样本集合\n",
    "def preprocess_testdata(content_line,stopwords,test_data,category,i=0):\n",
    "    for line in content_line:\n",
    "        try:\n",
    "            if len(line)>limits:\n",
    "                i+=1\n",
    "                if i<=maxss:\n",
    "                    segs=jieba.lcut(line)    #利用结巴分词进行中文分词\n",
    "                    segs=filter(lambda x:len(x)>1,segs)\n",
    "                    segs=filter(lambda x:x not in stopwords,segs)  #去掉停用词\n",
    "                    test_data.append(\"__label__\"+str(category)+\" , \"+\" \".join(segs))\n",
    "                else:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print ('line is',line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessTestData(saveDataFile,stopwords):\n",
    "    technology,sports,society,military,internality,house,home,finance,entertainment,car = loadData()    \n",
    "\n",
    "    #去停用词，生成数据集\n",
    "    sentences=[]\n",
    "    preprocess_testdata(technology,stopwords,sentences,cate_dic[\"technology\"])\n",
    "    preprocess_testdata(sports,stopwords,sentences,cate_dic[\"sports\"])\n",
    "    preprocess_testdata(society,stopwords,sentences,cate_dic[\"society\"])\n",
    "    #preprocess_testdata(military,stopwords,sentences,cate_dic[\"military\"])\n",
    "    preprocess_testdata(internality,stopwords,sentences,cate_dic[\"international\"])\n",
    "    #preprocess_testdata(house,stopwords,sentences,cate_dic[\"house\"])\n",
    "    preprocess_testdata(home,stopwords,sentences,cate_dic[\"home\"])\n",
    "    preprocess_testdata(finance,stopwords,sentences,cate_dic[\"finance\"])\n",
    "    preprocess_testdata(entertainment,stopwords,sentences,cate_dic[\"entertainment\"])\n",
    "    #preprocess_testdata(car,stopwords,sentences,cate_dic[\"car\"])\n",
    "\n",
    "    random.shuffle(sentences)    #做乱序处理，使得同类别的样本不至于扎堆\n",
    "    writeData(sentences,saveDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format...\n",
      "Writing Done!\n",
      "运行时间:0.53 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "stopwordsFile    = r\"./data/stopwords_NLP.txt\"\n",
    "stopwords        = getStopWords(stopwordsFile)\n",
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "preprocessTestData(saveTestDataFile,stopwords)\n",
    "t2 = time.time()\n",
    "print(\"运行时间:%0.2f\"%((t2 - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本长度： 140000\n",
      "测试集样本长度： 14000\n"
     ]
    }
   ],
   "source": [
    "### 计算文件中的样本个数 ### \n",
    "def get_examples(file,i=0):\n",
    "    with open(file,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            i+=1\n",
    "    return i\n",
    "print(\"训练集样本长度：\",get_examples(saveDataFile))\n",
    "print(\"测试集样本长度：\",get_examples(saveTestDataFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 fasttext做监督学习的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练140000个样本,迭代10000次,花费时间180.54 mins\n"
     ]
    }
   ],
   "source": [
    "# https://fasttext.cc/blog/2019/06/25/blog-post.html#2-you-were-using-the-unofficial-fasttext-module%22\n",
    "import time \n",
    "iters = 10000\n",
    "t1    = time.time()\n",
    "saveDataFile = r'./data/SavedData_Fasttext.txt'\n",
    "classifier   = fasttext.train_supervised(saveDataFile, lr=0.01, dim=100, epoch=iters,word_ngrams=2, loss='softmax')\n",
    "classifier.save_model(\"./data/fasttextmodel_file.bin\")\n",
    "\n",
    "print('训练%d个样本,迭代%d次,花费时间%0.2f'%(140000,iters,(time.time() - t1)/60),'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10000- 200mins/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0139971 , -0.06618144, -0.21357466, -0.02211528, -0.10686135,\n",
       "        0.0015256 ,  0.04707784,  0.09419458,  0.04111636,  0.00664799,\n",
       "       -0.06098977, -0.00103816, -0.09209058, -0.21497613,  0.04550919,\n",
       "        0.06377376,  0.0298731 , -0.58118773,  0.00915754, -0.20981802,\n",
       "        0.35174465, -0.03906257, -0.10356914,  0.06912361, -0.03498982,\n",
       "       -0.24295962,  0.02828444, -0.00908467,  0.03121671, -0.09233593,\n",
       "        0.20266858, -0.06681715, -0.2626985 , -0.00815375, -0.25781316,\n",
       "       -0.09126239,  0.06553549, -0.19335446,  0.07435322, -0.06199157,\n",
       "        0.09881378,  0.18628183,  0.03241622, -0.17996325,  0.13551721,\n",
       "       -0.03003756, -0.29824933,  0.13554122,  0.3000083 , -0.2479253 ,\n",
       "       -0.17519474,  0.02699481, -0.0187585 , -0.23846333,  0.01477795,\n",
       "       -0.07632194, -0.1276485 ,  0.2551831 , -0.0364856 , -0.00509563,\n",
       "        0.07927177, -0.0363422 ,  0.22622366,  0.32673833, -0.13128965,\n",
       "        0.11658817,  0.18361555,  0.16875482, -0.29766759, -0.01904358,\n",
       "       -0.20083319,  0.07284709,  0.05358569,  0.37118489, -0.17043896,\n",
       "        0.29077139, -0.14595392, -0.12029808,  0.16795419, -0.08985695,\n",
       "        0.38371348,  0.05272728, -0.35040158,  0.08339328, -0.10773014,\n",
       "        0.07273395, -0.06814157,  0.40109617,  0.13923258, -0.3528302 ,\n",
       "        0.03197211, -0.02568294, -0.10176292, -0.12023731, -0.21666139,\n",
       "       -0.34077695,  0.10291284, -0.09250785, -0.39591777,  0.01422861], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_word_vector('科技')  #获取某个词的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2重新加载模型 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.load_model(\"./data/fasttextmodel_file.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__7',\n",
       " '__label__9',\n",
       " '__label__1',\n",
       " '__label__5',\n",
       " '__label__3',\n",
       " '__label__8',\n",
       " '__label__2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result           = classifier.test(saveTestDataFile) # 这里应该使用另外一部分不同的样本做测试\n",
    "classifier.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__8', '__label__5']], array([[ 0.8129667 ,  0.14147736]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 预测一个样本，返回对应的几个可能Label及对应的可能的概率，k表示几个label,threshold 控制门限大于这个门限的才显示\n",
    "text_string = ['中国 国家 经济 生成 总值 位列 世界 第一']\n",
    "classifier.predict(text = text_string,k = 2,threshold = 0.1) # 输入是字符串 string \n",
    "#cate_dic = {'technology': 1, 'sports': 2, 'society': 3, 'military': 4, 'international': 5,\n",
    "#            'house':6,'home':7,'finance':8,'entertainment':9,'car':10}\n",
    "# 上面希望显示两个结果k=2，但是超过特定门限threshold = 0.1的只有一个结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Return the precision and recall score for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__1': {'f1score': 0.9987490617963473,\n",
       "  'precision': 0.99949924887331,\n",
       "  'recall': 0.998},\n",
       " '__label__2': {'f1score': 0.9995, 'precision': 0.9995, 'recall': 0.9995},\n",
       " '__label__3': {'f1score': 0.9731576378721327,\n",
       "  'precision': 0.9504289799809342,\n",
       "  'recall': 0.997},\n",
       " '__label__5': {'f1score': 0.998998998998999,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.998},\n",
       " '__label__7': {'f1score': 0.9705284552845529,\n",
       "  'precision': 0.9865702479338843,\n",
       "  'recall': 0.955},\n",
       " '__label__8': {'f1score': 0.9853977844914401,\n",
       "  'precision': 0.992393509127789,\n",
       "  'recall': 0.9785},\n",
       " '__label__9': {'f1score': 0.9992501874531368,\n",
       "  'precision': 0.999000499750125,\n",
       "  'recall': 0.9995}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "classifier.test_label(saveTestDataFile,k =1,threshold = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 返回模型的测试样本个数，准确率和召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 0.9893571428571428, 0.9893571428571428)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveTestDataFile     = r'./data/SavedTestData_Fasttext.txt'\n",
    "classifier.test(saveTestDataFile, k=1) # 这里应该使用另外一部分不同的样本做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _FastText in module fasttext.FastText object:\n",
      "\n",
      "class _FastText(builtins.object)\n",
      " |  This class defines the API to inspect models and should not be used to\n",
      " |  create objects. It will be returned by functions such as load_model or\n",
      " |  train.\n",
      " |  \n",
      " |  In general this API assumes to be given only unicode for Python2 and the\n",
      " |  Python3 equvalent called str for any string-like arguments. All unicode\n",
      " |  strings are then encoded as UTF-8 and fed to the fastText C++ API.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __getitem__(self, word)\n",
      " |  \n",
      " |  __init__(self, model_path=None, args=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_dimension(self)\n",
      " |      Get the dimension (size) of a lookup vector (hidden layer).\n",
      " |  \n",
      " |  get_input_matrix(self)\n",
      " |      Get a copy of the full input matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_input_vector(self, ind)\n",
      " |      Given an index, get the corresponding vector of the Input Matrix.\n",
      " |  \n",
      " |  get_labels(self, include_freq=False, on_unicode_error='strict')\n",
      " |      Get the entire list of labels of the dictionary optionally\n",
      " |      including the frequency of the individual labels. Unsupervised\n",
      " |      models use words as labels, which is why get_labels\n",
      " |      will call and return get_words for this type of\n",
      " |      model.\n",
      " |  \n",
      " |  get_line(self, text, on_unicode_error='strict')\n",
      " |      Split a line of text into words and labels. Labels must start with\n",
      " |      the prefix used to create the model (__label__ by default).\n",
      " |  \n",
      " |  get_output_matrix(self)\n",
      " |      Get a copy of the full output matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_sentence_vector(self, text)\n",
      " |      Given a string, get a single vector represenation. This function\n",
      " |      assumes to be given a single line of text. We split words on\n",
      " |      whitespace (space, newline, tab, vertical tab) and the control\n",
      " |      characters carriage return, formfeed and the null character.\n",
      " |  \n",
      " |  get_subword_id(self, subword)\n",
      " |      Given a subword, return the index (within input matrix) it hashes to.\n",
      " |  \n",
      " |  get_subwords(self, word, on_unicode_error='strict')\n",
      " |      Given a word, get the subwords and their indicies.\n",
      " |  \n",
      " |  get_word_id(self, word)\n",
      " |      Given a word, get the word id within the dictionary.\n",
      " |      Returns -1 if word is not in the dictionary.\n",
      " |  \n",
      " |  get_word_vector(self, word)\n",
      " |      Get the vector representation of word.\n",
      " |  \n",
      " |  get_words(self, include_freq=False, on_unicode_error='strict')\n",
      " |      Get the entire list of words of the dictionary optionally\n",
      " |      including the frequency of the individual words. This\n",
      " |      does not include any subwords. For that please consult\n",
      " |      the function get_subwords.\n",
      " |  \n",
      " |  is_quantized(self)\n",
      " |  \n",
      " |  predict(self, text, k=1, threshold=0.0, on_unicode_error='strict')\n",
      " |      Given a string, get a list of labels and a list of\n",
      " |      corresponding probabilities. k controls the number\n",
      " |      of returned labels. A choice of 5, will return the 5\n",
      " |      most probable labels. By default this returns only\n",
      " |      the most likely label and probability. threshold filters\n",
      " |      the returned labels by a threshold on probability. A\n",
      " |      choice of 0.5 will return labels with at least 0.5\n",
      " |      probability. k and threshold will be applied together to\n",
      " |      determine the returned labels.\n",
      " |      \n",
      " |      This function assumes to be given\n",
      " |      a single line of text. We split words on whitespace (space,\n",
      " |      newline, tab, vertical tab) and the control characters carriage\n",
      " |      return, formfeed and the null character.\n",
      " |      \n",
      " |      If the model is not supervised, this function will throw a ValueError.\n",
      " |      \n",
      " |      If given a list of strings, it will return a list of results as usually\n",
      " |      received for a single line of text.\n",
      " |  \n",
      " |  quantize(self, input=None, qout=False, cutoff=0, retrain=False, epoch=None, lr=None, thread=None, verbose=None, dsub=2, qnorm=False)\n",
      " |      Quantize the model reducing the size of the model and\n",
      " |      it's memory footprint.\n",
      " |  \n",
      " |  save_model(self, path)\n",
      " |      Save the model to the given path\n",
      " |  \n",
      " |  test(self, path, k=1)\n",
      " |      Evaluate supervised model using file given by path\n",
      " |  \n",
      " |  test_label(self, path, k=1, threshold=0.0)\n",
      " |      Return the precision and recall score for each label.\n",
      " |      \n",
      " |      The returned value is a dictionary, where the key is the label.\n",
      " |      For example:\n",
      " |      f.test_label(...)\n",
      " |      {'__label__italian-cuisine' : {'precision' : 0.7, 'recall' : 0.74}}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labels\n",
      " |  \n",
      " |  words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
